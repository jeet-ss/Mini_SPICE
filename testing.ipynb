{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-31 20:21:36.455332: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-31 20:21:36.493434: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-31 20:21:37.099396: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from utils.model import Spice_model\n",
    "from torchinfo import summary\n",
    "import pandas as pd\n",
    "from data_files.dataloader import MedleyDBLoader\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils.model import Spice_model, Spice_Encoder, Spice_Decoder\n",
    "from utils.model_types import Spice_model2, Spice_Decoder_lite, Deconv_block\n",
    "from utils.training_script import Trainer\n",
    "from optims.loss import Huber_loss, Recons_loss, Conf_loss\n",
    "from data_files.dataset import CQT_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph torch_jit (\n",
      "  %input[FLOAT, batch_sizex128]\n",
      ") initializers (\n",
      "  %enc_block.fc1.weight[FLOAT, 48x1024]\n",
      "  %enc_block.fc1.bias[FLOAT, 48]\n",
      "  %enc_block.fc2.weight[FLOAT, 1x48]\n",
      "  %enc_block.fc2.bias[FLOAT, 1]\n",
      "  %enc_block.conf_head.weight[FLOAT, 1x1024]\n",
      "  %enc_block.conf_head.bias[FLOAT, 1]\n",
      "  %dec_block.deconv_block1.deconv.weight[FLOAT, 512x256x3]\n",
      "  %dec_block.deconv_block1.batchNorm.weight[FLOAT, 256]\n",
      "  %dec_block.deconv_block1.batchNorm.bias[FLOAT, 256]\n",
      "  %dec_block.deconv_block2.deconv.weight[FLOAT, 256x256x3]\n",
      "  %dec_block.deconv_block3.deconv.weight[FLOAT, 256x256x3]\n",
      "  %dec_block.deconv_block4.deconv.weight[FLOAT, 256x128x3]\n",
      "  %dec_block.deconv_block4.batchNorm.weight[FLOAT, 128]\n",
      "  %dec_block.deconv_block4.batchNorm.bias[FLOAT, 128]\n",
      "  %dec_block.deconv_block5.deconv.weight[FLOAT, 128x64x3]\n",
      "  %dec_block.deconv_block5.batchNorm.weight[FLOAT, 64]\n",
      "  %dec_block.deconv_block5.batchNorm.bias[FLOAT, 64]\n",
      "  %dec_block.deconv_block6.deconv.weight[FLOAT, 64x32x3]\n",
      "  %dec_block.deconv_block6.batchNorm.weight[FLOAT, 32]\n",
      "  %dec_block.deconv_block6.batchNorm.bias[FLOAT, 32]\n",
      "  %dec_block.fc1.bias[FLOAT, 48]\n",
      "  %dec_block.fc2.bias[FLOAT, 1024]\n",
      "  %onnx::Conv_154[FLOAT, 64x1x3]\n",
      "  %onnx::Conv_157[FLOAT, 128x64x3]\n",
      "  %onnx::Conv_160[FLOAT, 256x128x3]\n",
      "  %onnx::Conv_163[FLOAT, 512x256x3]\n",
      "  %onnx::Conv_164[FLOAT, 512]\n",
      "  %onnx::Conv_166[FLOAT, 512x512x3]\n",
      "  %onnx::Conv_169[FLOAT, 512x512x3]\n",
      "  %onnx::MatMul_172[FLOAT, 1x48]\n",
      "  %onnx::MatMul_173[FLOAT, 48x1024]\n",
      ") {\n",
      "  %onnx::Conv_170 = Identity(%onnx::Conv_164)\n",
      "  %onnx::Conv_167 = Identity(%onnx::Conv_164)\n",
      "  %onnx::Conv_161 = Identity(%dec_block.deconv_block1.batchNorm.bias)\n",
      "  %onnx::Conv_158 = Identity(%dec_block.deconv_block4.batchNorm.bias)\n",
      "  %onnx::Conv_155 = Identity(%dec_block.deconv_block5.batchNorm.bias)\n",
      "  %dec_block.deconv_block6.batchNorm.running_var = Identity(%dec_block.deconv_block6.batchNorm.weight)\n",
      "  %dec_block.deconv_block6.batchNorm.running_mean = Identity(%dec_block.deconv_block6.batchNorm.bias)\n",
      "  %dec_block.deconv_block5.batchNorm.running_var = Identity(%dec_block.deconv_block5.batchNorm.weight)\n",
      "  %dec_block.deconv_block5.batchNorm.running_mean = Identity(%dec_block.deconv_block5.batchNorm.bias)\n",
      "  %dec_block.deconv_block4.batchNorm.running_var = Identity(%dec_block.deconv_block4.batchNorm.weight)\n",
      "  %dec_block.deconv_block4.batchNorm.running_mean = Identity(%dec_block.deconv_block4.batchNorm.bias)\n",
      "  %dec_block.deconv_block3.batchNorm.running_var = Identity(%dec_block.deconv_block1.batchNorm.weight)\n",
      "  %dec_block.deconv_block3.batchNorm.running_mean = Identity(%dec_block.deconv_block1.batchNorm.bias)\n",
      "  %dec_block.deconv_block3.batchNorm.bias = Identity(%dec_block.deconv_block1.batchNorm.bias)\n",
      "  %dec_block.deconv_block3.batchNorm.weight = Identity(%dec_block.deconv_block1.batchNorm.weight)\n",
      "  %dec_block.deconv_block2.batchNorm.running_var = Identity(%dec_block.deconv_block1.batchNorm.weight)\n",
      "  %dec_block.deconv_block2.batchNorm.running_mean = Identity(%dec_block.deconv_block1.batchNorm.bias)\n",
      "  %dec_block.deconv_block2.batchNorm.bias = Identity(%dec_block.deconv_block1.batchNorm.bias)\n",
      "  %dec_block.deconv_block2.batchNorm.weight = Identity(%dec_block.deconv_block1.batchNorm.weight)\n",
      "  %dec_block.deconv_block1.batchNorm.running_var = Identity(%dec_block.deconv_block1.batchNorm.weight)\n",
      "  %dec_block.deconv_block1.batchNorm.running_mean = Identity(%dec_block.deconv_block1.batchNorm.bias)\n",
      "  %/enc_block/Unsqueeze_output_0 = Unsqueeze[axes = [1]](%input)\n",
      "  %/enc_block/Shape_output_0 = Shape(%/enc_block/Unsqueeze_output_0)\n",
      "  %/enc_block/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/enc_block/Gather_output_0 = Gather[axis = 0](%/enc_block/Shape_output_0, %/enc_block/Constant_output_0)\n",
      "  %/enc_block/conv_block1/conv/Conv_output_0 = Conv[dilations = [1], group = 1, kernel_shape = [3], pads = [1, 1], strides = [1]](%/enc_block/Unsqueeze_output_0, %onnx::Conv_154, %onnx::Conv_155)\n",
      "  %/enc_block/conv_block1/relu/Relu_output_0 = Relu(%/enc_block/conv_block1/conv/Conv_output_0)\n",
      "  %/enc_block/conv_block1/maxPool/MaxPool_output_0 = MaxPool[ceil_mode = 0, kernel_shape = [3], pads = [1, 1], strides = [2]](%/enc_block/conv_block1/relu/Relu_output_0)\n",
      "  %/enc_block/conv_block2/conv/Conv_output_0 = Conv[dilations = [1], group = 1, kernel_shape = [3], pads = [1, 1], strides = [1]](%/enc_block/conv_block1/maxPool/MaxPool_output_0, %onnx::Conv_157, %onnx::Conv_158)\n",
      "  %/enc_block/conv_block2/relu/Relu_output_0 = Relu(%/enc_block/conv_block2/conv/Conv_output_0)\n",
      "  %/enc_block/conv_block2/maxPool/MaxPool_output_0 = MaxPool[ceil_mode = 0, kernel_shape = [3], pads = [1, 1], strides = [2]](%/enc_block/conv_block2/relu/Relu_output_0)\n",
      "  %/enc_block/conv_block3/conv/Conv_output_0 = Conv[dilations = [1], group = 1, kernel_shape = [3], pads = [1, 1], strides = [1]](%/enc_block/conv_block2/maxPool/MaxPool_output_0, %onnx::Conv_160, %onnx::Conv_161)\n",
      "  %/enc_block/conv_block3/relu/Relu_output_0 = Relu(%/enc_block/conv_block3/conv/Conv_output_0)\n",
      "  %/enc_block/conv_block3/maxPool/MaxPool_output_0 = MaxPool[ceil_mode = 0, kernel_shape = [3], pads = [1, 1], strides = [2]](%/enc_block/conv_block3/relu/Relu_output_0)\n",
      "  %/enc_block/conv_block4/conv/Conv_output_0 = Conv[dilations = [1], group = 1, kernel_shape = [3], pads = [1, 1], strides = [1]](%/enc_block/conv_block3/maxPool/MaxPool_output_0, %onnx::Conv_163, %onnx::Conv_164)\n",
      "  %/enc_block/conv_block4/relu/Relu_output_0 = Relu(%/enc_block/conv_block4/conv/Conv_output_0)\n",
      "  %/enc_block/conv_block4/maxPool/MaxPool_output_0 = MaxPool[ceil_mode = 0, kernel_shape = [3], pads = [1, 1], strides = [2]](%/enc_block/conv_block4/relu/Relu_output_0)\n",
      "  %/enc_block/conv_block5/conv/Conv_output_0 = Conv[dilations = [1], group = 1, kernel_shape = [3], pads = [1, 1], strides = [1]](%/enc_block/conv_block4/maxPool/MaxPool_output_0, %onnx::Conv_166, %onnx::Conv_167)\n",
      "  %/enc_block/conv_block5/relu/Relu_output_0 = Relu(%/enc_block/conv_block5/conv/Conv_output_0)\n",
      "  %/enc_block/conv_block5/maxPool/MaxPool_output_0 = MaxPool[ceil_mode = 0, kernel_shape = [3], pads = [1, 1], strides = [2]](%/enc_block/conv_block5/relu/Relu_output_0)\n",
      "  %/enc_block/conv_block6/conv/Conv_output_0 = Conv[dilations = [1], group = 1, kernel_shape = [3], pads = [1, 1], strides = [1]](%/enc_block/conv_block5/maxPool/MaxPool_output_0, %onnx::Conv_169, %onnx::Conv_170)\n",
      "  %/enc_block/conv_block6/relu/Relu_output_0 = Relu(%/enc_block/conv_block6/conv/Conv_output_0)\n",
      "  %/enc_block/conv_block6/maxPool/MaxPool_output_0 = MaxPool[ceil_mode = 0, kernel_shape = [3], pads = [1, 1], strides = [2]](%/enc_block/conv_block6/relu/Relu_output_0)\n",
      "  %/enc_block/Unsqueeze_1_output_0 = Unsqueeze[axes = [0]](%/enc_block/Gather_output_0)\n",
      "  %/enc_block/Constant_1_output_0 = Constant[value = <Tensor>]()\n",
      "  %/enc_block/Concat_output_0 = Concat[axis = 0](%/enc_block/Unsqueeze_1_output_0, %/enc_block/Constant_1_output_0)\n",
      "  %/enc_block/Reshape_output_0 = Reshape(%/enc_block/conv_block6/maxPool/MaxPool_output_0, %/enc_block/Concat_output_0)\n",
      "  %/enc_block/fc1/Gemm_output_0 = Gemm[alpha = 1, beta = 1, transB = 1](%/enc_block/Reshape_output_0, %enc_block.fc1.weight, %enc_block.fc1.bias)\n",
      "  %output = Gemm[alpha = 1, beta = 1, transB = 1](%/enc_block/fc1/Gemm_output_0, %enc_block.fc2.weight, %enc_block.fc2.bias)\n",
      "  %118 = Gemm[alpha = 1, beta = 1, transB = 1](%/enc_block/Reshape_output_0, %enc_block.conf_head.weight, %enc_block.conf_head.bias)\n",
      "  %/dec_block/Unsqueeze_output_0 = Unsqueeze[axes = [1]](%output)\n",
      "  %/dec_block/Shape_output_0 = Shape(%/dec_block/Unsqueeze_output_0)\n",
      "  %/dec_block/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/dec_block/Gather_output_0 = Gather[axis = 0](%/dec_block/Shape_output_0, %/dec_block/Constant_output_0)\n",
      "  %/dec_block/fc1/MatMul_output_0 = MatMul(%/dec_block/Unsqueeze_output_0, %onnx::MatMul_172)\n",
      "  %/dec_block/fc1/Add_output_0 = Add(%dec_block.fc1.bias, %/dec_block/fc1/MatMul_output_0)\n",
      "  %/dec_block/fc2/MatMul_output_0 = MatMul(%/dec_block/fc1/Add_output_0, %onnx::MatMul_173)\n",
      "  %/dec_block/fc2/Add_output_0 = Add(%dec_block.fc2.bias, %/dec_block/fc2/MatMul_output_0)\n",
      "  %/dec_block/Unsqueeze_1_output_0 = Unsqueeze[axes = [0]](%/dec_block/Gather_output_0)\n",
      "  %/dec_block/Constant_1_output_0 = Constant[value = <Tensor>]()\n",
      "  %/dec_block/Constant_2_output_0 = Constant[value = <Tensor>]()\n",
      "  %/dec_block/Concat_output_0 = Concat[axis = 0](%/dec_block/Unsqueeze_1_output_0, %/dec_block/Constant_1_output_0, %/dec_block/Constant_2_output_0)\n",
      "  %/dec_block/Reshape_output_0 = Reshape(%/dec_block/fc2/Add_output_0, %/dec_block/Concat_output_0)\n",
      "  %/dec_block/deconv_block1/deconv/ConvTranspose_output_0 = ConvTranspose[dilations = [1], group = 1, kernel_shape = [3], pads = [1, 1], strides = [1]](%/dec_block/Reshape_output_0, %dec_block.deconv_block1.deconv.weight)\n",
      "  %/dec_block/deconv_block1/batchNorm/BatchNormalization_output_0 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%/dec_block/deconv_block1/deconv/ConvTranspose_output_0, %dec_block.deconv_block1.batchNorm.weight, %dec_block.deconv_block1.batchNorm.bias, %dec_block.deconv_block1.batchNorm.running_mean, %dec_block.deconv_block1.batchNorm.running_var)\n",
      "  %/dec_block/deconv_block1/relu/Relu_output_0 = Relu(%/dec_block/deconv_block1/batchNorm/BatchNormalization_output_0)\n",
      "  %/dec_block/deconv_block2/deconv/ConvTranspose_output_0 = ConvTranspose[dilations = [1], group = 1, kernel_shape = [3], pads = [1, 1], strides = [1]](%/dec_block/deconv_block1/relu/Relu_output_0, %dec_block.deconv_block2.deconv.weight)\n",
      "  %/dec_block/deconv_block2/batchNorm/BatchNormalization_output_0 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%/dec_block/deconv_block2/deconv/ConvTranspose_output_0, %dec_block.deconv_block2.batchNorm.weight, %dec_block.deconv_block2.batchNorm.bias, %dec_block.deconv_block2.batchNorm.running_mean, %dec_block.deconv_block2.batchNorm.running_var)\n",
      "  %/dec_block/deconv_block2/relu/Relu_output_0 = Relu(%/dec_block/deconv_block2/batchNorm/BatchNormalization_output_0)\n",
      "  %/dec_block/deconv_block3/deconv/ConvTranspose_output_0 = ConvTranspose[dilations = [1], group = 1, kernel_shape = [3], pads = [1, 1], strides = [1]](%/dec_block/deconv_block2/relu/Relu_output_0, %dec_block.deconv_block3.deconv.weight)\n",
      "  %/dec_block/deconv_block3/batchNorm/BatchNormalization_output_0 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%/dec_block/deconv_block3/deconv/ConvTranspose_output_0, %dec_block.deconv_block3.batchNorm.weight, %dec_block.deconv_block3.batchNorm.bias, %dec_block.deconv_block3.batchNorm.running_mean, %dec_block.deconv_block3.batchNorm.running_var)\n",
      "  %/dec_block/deconv_block3/relu/Relu_output_0 = Relu(%/dec_block/deconv_block3/batchNorm/BatchNormalization_output_0)\n",
      "  %/dec_block/deconv_block4/deconv/ConvTranspose_output_0 = ConvTranspose[dilations = [1], group = 1, kernel_shape = [3], pads = [1, 1], strides = [1]](%/dec_block/deconv_block3/relu/Relu_output_0, %dec_block.deconv_block4.deconv.weight)\n",
      "  %/dec_block/deconv_block4/batchNorm/BatchNormalization_output_0 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%/dec_block/deconv_block4/deconv/ConvTranspose_output_0, %dec_block.deconv_block4.batchNorm.weight, %dec_block.deconv_block4.batchNorm.bias, %dec_block.deconv_block4.batchNorm.running_mean, %dec_block.deconv_block4.batchNorm.running_var)\n",
      "  %/dec_block/deconv_block4/relu/Relu_output_0 = Relu(%/dec_block/deconv_block4/batchNorm/BatchNormalization_output_0)\n",
      "  %/dec_block/deconv_block5/deconv/ConvTranspose_output_0 = ConvTranspose[dilations = [1], group = 1, kernel_shape = [3], pads = [1, 1], strides = [1]](%/dec_block/deconv_block4/relu/Relu_output_0, %dec_block.deconv_block5.deconv.weight)\n",
      "  %/dec_block/deconv_block5/batchNorm/BatchNormalization_output_0 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%/dec_block/deconv_block5/deconv/ConvTranspose_output_0, %dec_block.deconv_block5.batchNorm.weight, %dec_block.deconv_block5.batchNorm.bias, %dec_block.deconv_block5.batchNorm.running_mean, %dec_block.deconv_block5.batchNorm.running_var)\n",
      "  %/dec_block/deconv_block5/relu/Relu_output_0 = Relu(%/dec_block/deconv_block5/batchNorm/BatchNormalization_output_0)\n",
      "  %/dec_block/deconv_block6/deconv/ConvTranspose_output_0 = ConvTranspose[dilations = [1], group = 1, kernel_shape = [3], pads = [1, 1], strides = [1]](%/dec_block/deconv_block5/relu/Relu_output_0, %dec_block.deconv_block6.deconv.weight)\n",
      "  %/dec_block/deconv_block6/batchNorm/BatchNormalization_output_0 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%/dec_block/deconv_block6/deconv/ConvTranspose_output_0, %dec_block.deconv_block6.batchNorm.weight, %dec_block.deconv_block6.batchNorm.bias, %dec_block.deconv_block6.batchNorm.running_mean, %dec_block.deconv_block6.batchNorm.running_var)\n",
      "  %152 = Relu(%/dec_block/deconv_block6/batchNorm/BatchNormalization_output_0)\n",
      "  return %output, %118, %152\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "# Load the ONNX model\n",
    "model = onnx.load(\"check.onnx\")\n",
    "\n",
    "# Check that the model is well formed\n",
    "onnx.checker.check_model(model)\n",
    "\n",
    "# Print a human readable representation of the graph\n",
    "print(onnx.helper.printable_graph(model.graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP Error /onnxruntime_src/onnxruntime/core/providers/cuda/cuda_call.cc:121 std::conditional_t<THRW, void, onnxruntime::common::Status> onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*, const char*, int) [with ERRTYPE = cudaError; bool THRW = true; std::conditional_t<THRW, void, onnxruntime::common::Status> = void] /onnxruntime_src/onnxruntime/core/providers/cuda/cuda_call.cc:114 std::conditional_t<THRW, void, onnxruntime::common::Status> onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*, const char*, int) [with ERRTYPE = cudaError; bool THRW = true; std::conditional_t<THRW, void, onnxruntime::common::Status> = void] CUDA failure 710: device-side assert triggered ; GPU=0 ; hostname=pop-os ; file=/onnxruntime_src/onnxruntime/core/providers/cuda/cuda_execution_provider.cc ; line=241 ; expr=cudaDeviceSynchronize(); \n",
      "\n",
      " when using ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "Falling back to ['CUDAExecutionProvider', 'CPUExecutionProvider'] and retrying.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-31 21:09:51.187206836 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:614 CreateExecutionProviderInstance] Failed to create TensorrtExecutionProvider. Please reference https://onnxruntime.ai/docs/execution-providers/TensorRT-ExecutionProvider.html#requirements to ensure all dependencies are met.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "/onnxruntime_src/onnxruntime/core/providers/cuda/cuda_call.cc:121 std::conditional_t<THRW, void, onnxruntime::common::Status> onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*, const char*, int) [with ERRTYPE = cudaError; bool THRW = true; std::conditional_t<THRW, void, onnxruntime::common::Status> = void] /onnxruntime_src/onnxruntime/core/providers/cuda/cuda_call.cc:114 std::conditional_t<THRW, void, onnxruntime::common::Status> onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*, const char*, int) [with ERRTYPE = cudaError; bool THRW = true; std::conditional_t<THRW, void, onnxruntime::common::Status> = void] CUDA failure 710: device-side assert triggered ; GPU=0 ; hostname=pop-os ; file=/onnxruntime_src/onnxruntime/core/providers/cuda/cuda_execution_provider.cc ; line=241 ; expr=cudaDeviceSynchronize(); \n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:383\u001b[0m, in \u001b[0;36mInferenceSession.__init__\u001b[0;34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 383\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_inference_session(providers, provider_options, disabled_optimizers)\n\u001b[1;32m    384\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mRuntimeError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:435\u001b[0m, in \u001b[0;36mInferenceSession._create_inference_session\u001b[0;34m(self, providers, provider_options, disabled_optimizers)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[39m# initialize the C++ InferenceSession\u001b[39;00m\n\u001b[0;32m--> 435\u001b[0m sess\u001b[39m.\u001b[39;49minitialize_session(providers, provider_options, disabled_optimizers)\n\u001b[1;32m    437\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sess \u001b[39m=\u001b[39m sess\n",
      "\u001b[0;31mRuntimeError\u001b[0m: /onnxruntime_src/onnxruntime/core/providers/cuda/cuda_call.cc:121 std::conditional_t<THRW, void, onnxruntime::common::Status> onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*, const char*, int) [with ERRTYPE = cudaError; bool THRW = true; std::conditional_t<THRW, void, onnxruntime::common::Status> = void] /onnxruntime_src/onnxruntime/core/providers/cuda/cuda_call.cc:114 std::conditional_t<THRW, void, onnxruntime::common::Status> onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*, const char*, int) [with ERRTYPE = cudaError; bool THRW = true; std::conditional_t<THRW, void, onnxruntime::common::Status> = void] CUDA failure 710: device-side assert triggered ; GPU=0 ; hostname=pop-os ; file=/onnxruntime_src/onnxruntime/core/providers/cuda/cuda_execution_provider.cc ; line=241 ; expr=cudaDeviceSynchronize(); \n\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39monnxruntime\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mort\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m ort_session \u001b[39m=\u001b[39m ort\u001b[39m.\u001b[39;49mInferenceSession(\u001b[39m\"\u001b[39;49m\u001b[39mcheck.onnx\u001b[39;49m\u001b[39m\"\u001b[39;49m, providers\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mTensorrtExecutionProvider\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mCUDAExecutionProvider\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mCPUExecutionProvider\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m      5\u001b[0m outputs \u001b[39m=\u001b[39m ort_session\u001b[39m.\u001b[39mrun(\n\u001b[1;32m      6\u001b[0m     \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mactual_input_1\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandn(\u001b[39m1\u001b[39m, \u001b[39m128\u001b[39m)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32)},\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(outputs[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:394\u001b[0m, in \u001b[0;36mInferenceSession.__init__\u001b[0;34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    393\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m fallback_error:\n\u001b[0;32m--> 394\u001b[0m         \u001b[39mraise\u001b[39;00m fallback_error \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[39m# Fallback is disabled. Raise the original error.\u001b[39;00m\n\u001b[1;32m    396\u001b[0m \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:389\u001b[0m, in \u001b[0;36mInferenceSession.__init__\u001b[0;34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEP Error \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m when using \u001b[39m\u001b[39m{\u001b[39;00mproviders\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    388\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFalling back to \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fallback_providers\u001b[39m}\u001b[39;00m\u001b[39m and retrying.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 389\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_inference_session(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fallback_providers, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    390\u001b[0m \u001b[39m# Fallback only once.\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisable_fallback()\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:435\u001b[0m, in \u001b[0;36mInferenceSession._create_inference_session\u001b[0;34m(self, providers, provider_options, disabled_optimizers)\u001b[0m\n\u001b[1;32m    432\u001b[0m     disabled_optimizers \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(disabled_optimizers)\n\u001b[1;32m    434\u001b[0m \u001b[39m# initialize the C++ InferenceSession\u001b[39;00m\n\u001b[0;32m--> 435\u001b[0m sess\u001b[39m.\u001b[39;49minitialize_session(providers, provider_options, disabled_optimizers)\n\u001b[1;32m    437\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sess \u001b[39m=\u001b[39m sess\n\u001b[1;32m    438\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sess_options \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sess\u001b[39m.\u001b[39msession_options\n",
      "\u001b[0;31mRuntimeError\u001b[0m: /onnxruntime_src/onnxruntime/core/providers/cuda/cuda_call.cc:121 std::conditional_t<THRW, void, onnxruntime::common::Status> onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*, const char*, int) [with ERRTYPE = cudaError; bool THRW = true; std::conditional_t<THRW, void, onnxruntime::common::Status> = void] /onnxruntime_src/onnxruntime/core/providers/cuda/cuda_call.cc:114 std::conditional_t<THRW, void, onnxruntime::common::Status> onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*, const char*, int) [with ERRTYPE = cudaError; bool THRW = true; std::conditional_t<THRW, void, onnxruntime::common::Status> = void] CUDA failure 710: device-side assert triggered ; GPU=0 ; hostname=pop-os ; file=/onnxruntime_src/onnxruntime/core/providers/cuda/cuda_execution_provider.cc ; line=241 ; expr=cudaDeviceSynchronize(); \n\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "ort_session = ort.InferenceSession(\"check.onnx\", providers=['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
    "\n",
    "outputs = ort_session.run(\n",
    "    None,\n",
    "    {\"actual_input_1\": np.random.randn(1, 128).astype(np.float32)},\n",
    ")\n",
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MaxUnpool1d.forward() missing 1 required positional argument: 'indices'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m mod \u001b[39m=\u001b[39m Deconv_block()\n\u001b[0;32m----> 2\u001b[0m ss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49mtrace(mod, torch\u001b[39m.\u001b[39;49mrand(\u001b[39m10\u001b[39;49m, \u001b[39m64\u001b[39;49m, \u001b[39m64\u001b[39;49m))\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/torch/jit/_trace.py:794\u001b[0m, in \u001b[0;36mtrace\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_kwarg_inputs, _store_inputs)\u001b[0m\n\u001b[1;32m    792\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    793\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mexample_kwarg_inputs should be a dict\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 794\u001b[0m     \u001b[39mreturn\u001b[39;00m trace_module(\n\u001b[1;32m    795\u001b[0m         func,\n\u001b[1;32m    796\u001b[0m         {\u001b[39m\"\u001b[39;49m\u001b[39mforward\u001b[39;49m\u001b[39m\"\u001b[39;49m: example_inputs},\n\u001b[1;32m    797\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    798\u001b[0m         check_trace,\n\u001b[1;32m    799\u001b[0m         wrap_check_inputs(check_inputs),\n\u001b[1;32m    800\u001b[0m         check_tolerance,\n\u001b[1;32m    801\u001b[0m         strict,\n\u001b[1;32m    802\u001b[0m         _force_outplace,\n\u001b[1;32m    803\u001b[0m         _module_class,\n\u001b[1;32m    804\u001b[0m         example_inputs_is_kwarg\u001b[39m=\u001b[39;49m\u001b[39misinstance\u001b[39;49m(example_kwarg_inputs, \u001b[39mdict\u001b[39;49m),\n\u001b[1;32m    805\u001b[0m         _store_inputs\u001b[39m=\u001b[39;49m_store_inputs\n\u001b[1;32m    806\u001b[0m     )\n\u001b[1;32m    807\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    808\u001b[0m     \u001b[39mhasattr\u001b[39m(func, \u001b[39m\"\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    809\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(func\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule)\n\u001b[1;32m    810\u001b[0m     \u001b[39mand\u001b[39;00m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mforward\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    811\u001b[0m ):\n\u001b[1;32m    812\u001b[0m     \u001b[39mif\u001b[39;00m example_inputs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/torch/jit/_trace.py:1056\u001b[0m, in \u001b[0;36mtrace_module\u001b[0;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_inputs_is_kwarg, _store_inputs)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1055\u001b[0m     example_inputs \u001b[39m=\u001b[39m make_tuple(example_inputs)\n\u001b[0;32m-> 1056\u001b[0m     module\u001b[39m.\u001b[39;49m_c\u001b[39m.\u001b[39;49m_create_method_from_trace(\n\u001b[1;32m   1057\u001b[0m         method_name,\n\u001b[1;32m   1058\u001b[0m         func,\n\u001b[1;32m   1059\u001b[0m         example_inputs,\n\u001b[1;32m   1060\u001b[0m         var_lookup_fn,\n\u001b[1;32m   1061\u001b[0m         strict,\n\u001b[1;32m   1062\u001b[0m         _force_outplace,\n\u001b[1;32m   1063\u001b[0m         argument_names,\n\u001b[1;32m   1064\u001b[0m         _store_inputs\n\u001b[1;32m   1065\u001b[0m     )\n\u001b[1;32m   1067\u001b[0m check_trace_method \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_c\u001b[39m.\u001b[39m_get_method(method_name)\n\u001b[1;32m   1069\u001b[0m \u001b[39m# Check the trace against new traces created from user-specified inputs\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/torch/nn/modules/module.py:1488\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1486\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1488\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1489\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1490\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m~/Documents/ai_fau_study/Sose23/MPA_project/Mini_SPICE/utils/modelExtra.py:23\u001b[0m, in \u001b[0;36mDeconv_block.forward\u001b[0;34m(self, input_1D)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_1D):\n\u001b[1;32m     21\u001b[0m     \n\u001b[1;32m     22\u001b[0m     \u001b[39m# Unpool\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     input_1D \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munpool(input_1D)\n\u001b[1;32m     24\u001b[0m     \u001b[39m# else:\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[39m#     print(\"no unpooling\", self.unPooling, unpool_mat is None)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[39m# Transpose conv\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     input_1D \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeconv(input_1D)\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/torch/nn/modules/module.py:1488\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1486\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1488\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1489\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1490\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "\u001b[0;31mTypeError\u001b[0m: MaxUnpool1d.forward() missing 1 required positional argument: 'indices'"
     ]
    }
   ],
   "source": [
    "mod = Deconv_block()\n",
    "ss = torch.jit.trace(mod, torch.rand(10, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    input_1D: Tensor) -> Tensor:\n",
      "  relu = self.relu\n",
      "  batchNorm = self.batchNorm\n",
      "  deconv = self.deconv\n",
      "  _0 = (batchNorm).forward((deconv).forward(input_1D, ), )\n",
      "  return (relu).forward(_0, )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ss.code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "torch.manual_seed(191009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def foo_sum(x,y):\n",
    "    return x+ y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCell(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyCell, self).__init__()\n",
    "        self.linear = torch.nn.Linear(4, 4)\n",
    "        \n",
    "\n",
    "    def forward(self, x, h):\n",
    "        new_h = torch.tanh(self.linear(x) + h)\n",
    "        return foo_sum(new_h, new_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecisionGate(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        if x.sum() > 0:\n",
    "            return x\n",
    "        else:\n",
    "            return -x\n",
    "\n",
    "class MyCell2(torch.nn.Module):\n",
    "    def __init__(self, dg):\n",
    "        super(MyCell2, self).__init__()\n",
    "        self.dg = dg\n",
    "        self.linear = torch.nn.Linear(4, 4)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        new_h = torch.tanh(self.dg(self.linear(x)) + h)\n",
    "        return new_h, new_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_cell = MyCell()\n",
    "x = torch.rand(3, 4)\n",
    "h = torch.rand(3, 4)\n",
    "print(my_cell(x, h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracd MyCell(\n",
      "  original_name=MyCell\n",
      "  (linear): Linear(original_name=Linear)\n",
      ") end\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7430,  1.4186,  1.5269,  0.7439],\n",
       "        [ 0.6736, -0.4245,  0.9019, -0.2510],\n",
       "        [ 1.0948,  0.4671,  0.8218,  0.3636]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_cell = MyCell()\n",
    "x, h = torch.rand(3, 4), torch.rand(3, 4)\n",
    "traced_cell = torch.jit.trace(my_cell, (x, h))\n",
    "print(\"tracd\",traced_cell,\"end\")\n",
    "traced_cell(x, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    x: Tensor,\n",
      "    h: Tensor) -> Tensor:\n",
      "  linear = self.linear\n",
      "  new_h = torch.tanh(torch.add((linear).forward(x, ), h))\n",
      "  return __torch__.foo_sum(new_h, new_h, )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print(traced_cell.graph)\n",
    "print(traced_cell.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_cell = MyCell2(MyDecisionGate())\n",
    "traced_cell = torch.jit.trace(my_cell, (x, h))\n",
    "\n",
    "print(traced_cell.dg.code)\n",
    "print(traced_cell.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripted_gate = torch.jit.script(MyDecisionGate())\n",
    "\n",
    "my_cell = MyCell2(scripted_gate)\n",
    "scripted_cell = torch.jit.script(my_cell)\n",
    "\n",
    "print(scripted_gate.code)\n",
    "print(scripted_cell.code)\n",
    "print(scripted_cell(x, h))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyRNNLoop(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyRNNLoop, self).__init__()\n",
    "        self.cell = torch.jit.script(MyCell2(scripted_gate))\n",
    "        self.unpool = nn.MaxPool1d(kernel_size=3, stride=1, padding=1)\n",
    "    \n",
    "    def forward(self, xs):\n",
    "        h, y = torch.zeros(3, 4), torch.zeros(3, 4)\n",
    "        for i in range(xs.size(0)):\n",
    "            y, h = self.unpool(xs[i], h)\n",
    "        return y, h\n",
    "\n",
    "rnn_loop = torch.jit.script( MyRNNLoop())\n",
    "print(rnn_loop.code)\n",
    "print(rnn_loop.cell.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrapRNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WrapRNN, self).__init__()\n",
    "        self.loop = torch.jit.script(MyRNNLoop())\n",
    "\n",
    "    def forward(self, xs):\n",
    "        y, h = self.loop(xs)\n",
    "        temp=0\n",
    "        for i in h:\n",
    "            temp+=y\n",
    "        return torch.relu(y)+temp\n",
    "\n",
    "traced = torch.jit.script(WrapRNN())\n",
    "print(traced.loop.cell.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class convBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.batch = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxPool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1, return_indices=True)\n",
    "    def forward(self, input_1D):\n",
    "        input_1D = self.conv(input_1D)\n",
    "        input_1D = self.relu(self.batch(input_1D))\n",
    "        input_1D, indx_mat = self.maxPool(input_1D)\n",
    "\n",
    "        return input_1D, indx_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class deconvBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.deconv = nn.ConvTranspose1d(64, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.batchNorm = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.unpool = nn.MaxUnpool1d(kernel_size=3, stride=1, padding=0)\n",
    "    \n",
    "    def forward(self, input_1D1, idmat):\n",
    "        input_1D = self.unpool(input_1D1, idmat)\n",
    "        input_1D = self.deconv(input_1D)\n",
    "        # batch norm\n",
    "        input_1D = self.batchNorm(input_1D)\n",
    "        # relu \n",
    "        input_1D = self.relu(input_1D)\n",
    "        return input_1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class modelFull(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bll = convBlock()\n",
    "        self.deconv = deconvBlock()\n",
    "    def forward(self, x):\n",
    "            xx, y = self.bll(x)\n",
    "            xz = self.deconv(xx, y)\n",
    "            return xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Spice_model2(\n",
       "  original_name=Spice_model2\n",
       "  (enc_block): Spice_Encoder(\n",
       "    original_name=Spice_Encoder\n",
       "    (conv_block1): Conv_block(\n",
       "      original_name=Conv_block\n",
       "      (conv): Conv1d(original_name=Conv1d)\n",
       "      (batch): BatchNorm1d(original_name=BatchNorm1d)\n",
       "      (relu): ReLU(original_name=ReLU)\n",
       "      (maxPool): MaxPool1d(original_name=MaxPool1d)\n",
       "    )\n",
       "    (conv_block2): Conv_block(\n",
       "      original_name=Conv_block\n",
       "      (conv): Conv1d(original_name=Conv1d)\n",
       "      (batch): BatchNorm1d(original_name=BatchNorm1d)\n",
       "      (relu): ReLU(original_name=ReLU)\n",
       "      (maxPool): MaxPool1d(original_name=MaxPool1d)\n",
       "    )\n",
       "    (conv_block3): Conv_block(\n",
       "      original_name=Conv_block\n",
       "      (conv): Conv1d(original_name=Conv1d)\n",
       "      (batch): BatchNorm1d(original_name=BatchNorm1d)\n",
       "      (relu): ReLU(original_name=ReLU)\n",
       "      (maxPool): MaxPool1d(original_name=MaxPool1d)\n",
       "    )\n",
       "    (conv_block4): Conv_block(\n",
       "      original_name=Conv_block\n",
       "      (conv): Conv1d(original_name=Conv1d)\n",
       "      (batch): BatchNorm1d(original_name=BatchNorm1d)\n",
       "      (relu): ReLU(original_name=ReLU)\n",
       "      (maxPool): MaxPool1d(original_name=MaxPool1d)\n",
       "    )\n",
       "    (conv_block5): Conv_block(\n",
       "      original_name=Conv_block\n",
       "      (conv): Conv1d(original_name=Conv1d)\n",
       "      (batch): BatchNorm1d(original_name=BatchNorm1d)\n",
       "      (relu): ReLU(original_name=ReLU)\n",
       "      (maxPool): MaxPool1d(original_name=MaxPool1d)\n",
       "    )\n",
       "    (conv_block6): Conv_block(\n",
       "      original_name=Conv_block\n",
       "      (conv): Conv1d(original_name=Conv1d)\n",
       "      (batch): BatchNorm1d(original_name=BatchNorm1d)\n",
       "      (relu): ReLU(original_name=ReLU)\n",
       "      (maxPool): MaxPool1d(original_name=MaxPool1d)\n",
       "    )\n",
       "    (fc1): Linear(original_name=Linear)\n",
       "    (fc2): Linear(original_name=Linear)\n",
       "    (conf_head): Linear(original_name=Linear)\n",
       "  )\n",
       "  (dec_block): Spice_Decoder_lite(\n",
       "    original_name=Spice_Decoder_lite\n",
       "    (deconv_block1): Deconv_block(\n",
       "      original_name=Deconv_block\n",
       "      (deconv): ConvTranspose1d(original_name=ConvTranspose1d)\n",
       "      (batchNorm): BatchNorm1d(original_name=BatchNorm1d)\n",
       "      (relu): ReLU(original_name=ReLU)\n",
       "      (unpool): MaxUnpool1d(original_name=MaxUnpool1d)\n",
       "    )\n",
       "    (deconv_block2): Deconv_block(\n",
       "      original_name=Deconv_block\n",
       "      (deconv): ConvTranspose1d(original_name=ConvTranspose1d)\n",
       "      (batchNorm): BatchNorm1d(original_name=BatchNorm1d)\n",
       "      (relu): ReLU(original_name=ReLU)\n",
       "      (unpool): MaxUnpool1d(original_name=MaxUnpool1d)\n",
       "    )\n",
       "    (deconv_block3): Deconv_block(\n",
       "      original_name=Deconv_block\n",
       "      (deconv): ConvTranspose1d(original_name=ConvTranspose1d)\n",
       "      (batchNorm): BatchNorm1d(original_name=BatchNorm1d)\n",
       "      (relu): ReLU(original_name=ReLU)\n",
       "      (unpool): MaxUnpool1d(original_name=MaxUnpool1d)\n",
       "    )\n",
       "    (deconv_block4): Deconv_block(\n",
       "      original_name=Deconv_block\n",
       "      (deconv): ConvTranspose1d(original_name=ConvTranspose1d)\n",
       "      (batchNorm): BatchNorm1d(original_name=BatchNorm1d)\n",
       "      (relu): ReLU(original_name=ReLU)\n",
       "      (unpool): MaxUnpool1d(original_name=MaxUnpool1d)\n",
       "    )\n",
       "    (deconv_block5): Deconv_block(\n",
       "      original_name=Deconv_block\n",
       "      (deconv): ConvTranspose1d(original_name=ConvTranspose1d)\n",
       "      (batchNorm): BatchNorm1d(original_name=BatchNorm1d)\n",
       "      (relu): ReLU(original_name=ReLU)\n",
       "      (unpool): MaxUnpool1d(original_name=MaxUnpool1d)\n",
       "    )\n",
       "    (deconv_block6): Deconv_block(\n",
       "      original_name=Deconv_block\n",
       "      (deconv): ConvTranspose1d(original_name=ConvTranspose1d)\n",
       "      (batchNorm): BatchNorm1d(original_name=BatchNorm1d)\n",
       "      (relu): ReLU(original_name=ReLU)\n",
       "      (unpool): MaxUnpool1d(original_name=MaxUnpool1d)\n",
       "    )\n",
       "    (fc1): Linear(original_name=Linear)\n",
       "    (fc2): Linear(original_name=Linear)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.jit.trace(Spice_model2(), torch.rand(1,  128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "modelFull                                [2, 32, 66]               --\n",
       "convBlock: 1-1                         [2, 64, 64]               --\n",
       "    Conv1d: 2-1                       [2, 64, 128]              192\n",
       "    BatchNorm1d: 2-2                  [2, 64, 128]              128\n",
       "    ReLU: 2-3                         [2, 64, 128]              --\n",
       "    MaxPool1d: 2-4                    [2, 64, 64]               --\n",
       "deconvBlock: 1-2                       [2, 32, 66]               --\n",
       "    MaxUnpool1d: 2-5                  [2, 64, 66]               --\n",
       "    ConvTranspose1d: 2-6              [2, 32, 66]               6,144\n",
       "    BatchNorm1d: 2-7                  [2, 32, 66]               64\n",
       "    ReLU: 2-8                         [2, 32, 66]               --\n",
       "==========================================================================================\n",
       "Total params: 6,528\n",
       "Trainable params: 6,528\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.86\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.33\n",
       "Params size (MB): 0.03\n",
       "Estimated Total Size (MB): 0.36\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = modelFull()\n",
    "summary(s, input_size=[2, 1, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.jit.script(modelFull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spice = Spice_Decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.jit.script(spice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 16000\n",
    "med = MedleyDBLoader(fs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = 1110.0644999999954 \n",
    "f2 = -5.684341886080802e-14\n",
    "dive = f1/f2\n",
    "dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log2(dive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pd = pd.read_pickle('./CQT_data/MIR1k.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnp = np.load('./CQT_data/MedleyDB.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnp = torch.toTe(nnp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_pd = pd.DataFrame(data=nnp) \n",
    "train, val = train_test_split(data_pd, train_size=0.8, test_size=0.2, random_state=1)\n",
    "print(\"train shape: \", train.shape)\n",
    "train_batches = DataLoader(CQT_Dataset(data=train, mode='train'), batch_size=64, shuffle=True)\n",
    "print(\"train_batch NUmber: \", len(train_batches))\n",
    "diff, slice1, slice2, f0 = next(iter(train_batches))\n",
    "print(f\"diff batch shape: {diff.size()}\")\n",
    "print(f\"slice1 batch shape: {slice1.size()}\")\n",
    "print(f\"slice2 batch shape: {f0.size()}\")\n",
    "\n",
    "spice = Spice_model()\n",
    "\n",
    "\n",
    "for b in train_batches:\n",
    "    print(\"batch shape:\", b[0].shape)\n",
    "    pitch_diff, x_1, x_2, f0 = b\n",
    "    x_1 = x_1.type(torch.FloatTensor)\n",
    "    #print(x_1.shape, x_1.type())\n",
    "    a, x, y = spice(x_1)\n",
    "    print(a.size(), x.size(), y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx=torch.arange(0, 25, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = xx.reshape((5,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./CQT_data/MIR1k.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(0,25,1).reshape(5,5)[:,-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(0,15,1).reshape(5,3)\n",
    "a = np.hstack((a, np.array([0.25,0.7,0,0.5,0]).reshape(-1,1)))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(a)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows of cqt where label (last) column is zero\n",
    "df.drop(df.loc[df.iloc[:, -1]==0].index, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(df.iloc[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "isIntList() INTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/core/ivalue_inl.h\":1938, please report a bug to PyTorch. Expected IntList but got Int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49mscript(Spice_model2())\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/torch/jit/_script.py:1284\u001b[0m, in \u001b[0;36mscript\u001b[0;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule):\n\u001b[1;32m   1283\u001b[0m     obj \u001b[39m=\u001b[39m call_prepare_scriptable_func(obj)\n\u001b[0;32m-> 1284\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39m_recursive\u001b[39m.\u001b[39mcreate_script_module(\n\u001b[1;32m   1285\u001b[0m         obj, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39m_recursive\u001b[39m.\u001b[39minfer_methods_to_compile\n\u001b[1;32m   1286\u001b[0m     )\n\u001b[1;32m   1288\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, \u001b[39mdict\u001b[39m):\n\u001b[1;32m   1289\u001b[0m     \u001b[39mreturn\u001b[39;00m create_script_dict(obj)\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/torch/jit/_recursive.py:480\u001b[0m, in \u001b[0;36mcreate_script_module\u001b[0;34m(nn_module, stubs_fn, share_types, is_tracing)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tracing:\n\u001b[1;32m    479\u001b[0m     AttributeTypeIsSupportedChecker()\u001b[39m.\u001b[39mcheck(nn_module)\n\u001b[0;32m--> 480\u001b[0m \u001b[39mreturn\u001b[39;00m create_script_module_impl(nn_module, concrete_type, stubs_fn)\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/torch/jit/_recursive.py:542\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    539\u001b[0m     script_module\u001b[39m.\u001b[39m_concrete_type \u001b[39m=\u001b[39m concrete_type\n\u001b[1;32m    541\u001b[0m \u001b[39m# Actually create the ScriptModule, initializing it with the function we just defined\u001b[39;00m\n\u001b[0;32m--> 542\u001b[0m script_module \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49mRecursiveScriptModule\u001b[39m.\u001b[39;49m_construct(cpp_module, init_fn)\n\u001b[1;32m    544\u001b[0m \u001b[39m# Compile methods if necessary\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[39mif\u001b[39;00m concrete_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m concrete_type_store\u001b[39m.\u001b[39mmethods_compiled:\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/torch/jit/_script.py:614\u001b[0m, in \u001b[0;36mRecursiveScriptModule._construct\u001b[0;34m(cpp_module, init_fn)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    602\u001b[0m \u001b[39mConstruct a RecursiveScriptModule that's ready for use. PyTorch\u001b[39;00m\n\u001b[1;32m    603\u001b[0m \u001b[39mcode should use this to construct a RecursiveScriptModule instead\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[39m    init_fn:  Lambda that initializes the RecursiveScriptModule passed to it.\u001b[39;00m\n\u001b[1;32m    612\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    613\u001b[0m script_module \u001b[39m=\u001b[39m RecursiveScriptModule(cpp_module)\n\u001b[0;32m--> 614\u001b[0m init_fn(script_module)\n\u001b[1;32m    616\u001b[0m \u001b[39m# Finalize the ScriptModule: replace the nn.Module state with our\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[39m# custom implementations and flip the _initializing bit.\u001b[39;00m\n\u001b[1;32m    618\u001b[0m RecursiveScriptModule\u001b[39m.\u001b[39m_finalize_scriptmodule(script_module)\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/torch/jit/_recursive.py:520\u001b[0m, in \u001b[0;36mcreate_script_module_impl.<locals>.init_fn\u001b[0;34m(script_module)\u001b[0m\n\u001b[1;32m    517\u001b[0m     scripted \u001b[39m=\u001b[39m orig_value\n\u001b[1;32m    518\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     \u001b[39m# always reuse the provided stubs_fn to infer the methods to compile\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m     scripted \u001b[39m=\u001b[39m create_script_module_impl(orig_value, sub_concrete_type, stubs_fn)\n\u001b[1;32m    522\u001b[0m cpp_module\u001b[39m.\u001b[39msetattr(name, scripted)\n\u001b[1;32m    523\u001b[0m script_module\u001b[39m.\u001b[39m_modules[name] \u001b[39m=\u001b[39m scripted\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/torch/jit/_recursive.py:542\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    539\u001b[0m     script_module\u001b[39m.\u001b[39m_concrete_type \u001b[39m=\u001b[39m concrete_type\n\u001b[1;32m    541\u001b[0m \u001b[39m# Actually create the ScriptModule, initializing it with the function we just defined\u001b[39;00m\n\u001b[0;32m--> 542\u001b[0m script_module \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49mRecursiveScriptModule\u001b[39m.\u001b[39;49m_construct(cpp_module, init_fn)\n\u001b[1;32m    544\u001b[0m \u001b[39m# Compile methods if necessary\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[39mif\u001b[39;00m concrete_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m concrete_type_store\u001b[39m.\u001b[39mmethods_compiled:\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/torch/jit/_script.py:614\u001b[0m, in \u001b[0;36mRecursiveScriptModule._construct\u001b[0;34m(cpp_module, init_fn)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    602\u001b[0m \u001b[39mConstruct a RecursiveScriptModule that's ready for use. PyTorch\u001b[39;00m\n\u001b[1;32m    603\u001b[0m \u001b[39mcode should use this to construct a RecursiveScriptModule instead\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[39m    init_fn:  Lambda that initializes the RecursiveScriptModule passed to it.\u001b[39;00m\n\u001b[1;32m    612\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    613\u001b[0m script_module \u001b[39m=\u001b[39m RecursiveScriptModule(cpp_module)\n\u001b[0;32m--> 614\u001b[0m init_fn(script_module)\n\u001b[1;32m    616\u001b[0m \u001b[39m# Finalize the ScriptModule: replace the nn.Module state with our\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[39m# custom implementations and flip the _initializing bit.\u001b[39;00m\n\u001b[1;32m    618\u001b[0m RecursiveScriptModule\u001b[39m.\u001b[39m_finalize_scriptmodule(script_module)\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/torch/jit/_recursive.py:520\u001b[0m, in \u001b[0;36mcreate_script_module_impl.<locals>.init_fn\u001b[0;34m(script_module)\u001b[0m\n\u001b[1;32m    517\u001b[0m     scripted \u001b[39m=\u001b[39m orig_value\n\u001b[1;32m    518\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     \u001b[39m# always reuse the provided stubs_fn to infer the methods to compile\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m     scripted \u001b[39m=\u001b[39m create_script_module_impl(orig_value, sub_concrete_type, stubs_fn)\n\u001b[1;32m    522\u001b[0m cpp_module\u001b[39m.\u001b[39msetattr(name, scripted)\n\u001b[1;32m    523\u001b[0m script_module\u001b[39m.\u001b[39m_modules[name] \u001b[39m=\u001b[39m scripted\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/torch/jit/_recursive.py:542\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    539\u001b[0m     script_module\u001b[39m.\u001b[39m_concrete_type \u001b[39m=\u001b[39m concrete_type\n\u001b[1;32m    541\u001b[0m \u001b[39m# Actually create the ScriptModule, initializing it with the function we just defined\u001b[39;00m\n\u001b[0;32m--> 542\u001b[0m script_module \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49mRecursiveScriptModule\u001b[39m.\u001b[39;49m_construct(cpp_module, init_fn)\n\u001b[1;32m    544\u001b[0m \u001b[39m# Compile methods if necessary\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[39mif\u001b[39;00m concrete_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m concrete_type_store\u001b[39m.\u001b[39mmethods_compiled:\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/torch/jit/_script.py:614\u001b[0m, in \u001b[0;36mRecursiveScriptModule._construct\u001b[0;34m(cpp_module, init_fn)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    602\u001b[0m \u001b[39mConstruct a RecursiveScriptModule that's ready for use. PyTorch\u001b[39;00m\n\u001b[1;32m    603\u001b[0m \u001b[39mcode should use this to construct a RecursiveScriptModule instead\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[39m    init_fn:  Lambda that initializes the RecursiveScriptModule passed to it.\u001b[39;00m\n\u001b[1;32m    612\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    613\u001b[0m script_module \u001b[39m=\u001b[39m RecursiveScriptModule(cpp_module)\n\u001b[0;32m--> 614\u001b[0m init_fn(script_module)\n\u001b[1;32m    616\u001b[0m \u001b[39m# Finalize the ScriptModule: replace the nn.Module state with our\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[39m# custom implementations and flip the _initializing bit.\u001b[39;00m\n\u001b[1;32m    618\u001b[0m RecursiveScriptModule\u001b[39m.\u001b[39m_finalize_scriptmodule(script_module)\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/torch/jit/_recursive.py:520\u001b[0m, in \u001b[0;36mcreate_script_module_impl.<locals>.init_fn\u001b[0;34m(script_module)\u001b[0m\n\u001b[1;32m    517\u001b[0m     scripted \u001b[39m=\u001b[39m orig_value\n\u001b[1;32m    518\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     \u001b[39m# always reuse the provided stubs_fn to infer the methods to compile\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m     scripted \u001b[39m=\u001b[39m create_script_module_impl(orig_value, sub_concrete_type, stubs_fn)\n\u001b[1;32m    522\u001b[0m cpp_module\u001b[39m.\u001b[39msetattr(name, scripted)\n\u001b[1;32m    523\u001b[0m script_module\u001b[39m.\u001b[39m_modules[name] \u001b[39m=\u001b[39m scripted\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/torch/jit/_recursive.py:546\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[39m# Compile methods if necessary\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[39mif\u001b[39;00m concrete_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m concrete_type_store\u001b[39m.\u001b[39mmethods_compiled:\n\u001b[0;32m--> 546\u001b[0m     create_methods_and_properties_from_stubs(concrete_type, method_stubs, property_stubs)\n\u001b[1;32m    547\u001b[0m     \u001b[39m# Create hooks after methods to ensure no name collisions between hooks and methods.\u001b[39;00m\n\u001b[1;32m    548\u001b[0m     \u001b[39m# If done before, hooks can overshadow methods that aren't exported.\u001b[39;00m\n\u001b[1;32m    549\u001b[0m     create_hooks_from_stubs(concrete_type, hook_stubs, pre_hook_stubs)\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/torch/jit/_recursive.py:397\u001b[0m, in \u001b[0;36mcreate_methods_and_properties_from_stubs\u001b[0;34m(concrete_type, method_stubs, property_stubs)\u001b[0m\n\u001b[1;32m    394\u001b[0m property_defs \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mdef_ \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m property_stubs]\n\u001b[1;32m    395\u001b[0m property_rcbs \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mresolution_callback \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m property_stubs]\n\u001b[0;32m--> 397\u001b[0m concrete_type\u001b[39m.\u001b[39;49m_create_methods_and_properties(property_defs, property_rcbs, method_defs, method_rcbs, method_defaults)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: isIntList() INTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/core/ivalue_inl.h\":1938, please report a bug to PyTorch. Expected IntList but got Int"
     ]
    }
   ],
   "source": [
    "torch.jit.script(Spice_model2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Spice_model2                             [10, 1]                   --\n",
       "Spice_Encoder: 1-1                     [10, 1]                   --\n",
       "    Conv_block: 2-1                   [10, 64, 64]              --\n",
       "        Conv1d: 3-1                  [10, 64, 128]             192\n",
       "        BatchNorm1d: 3-2             [10, 64, 128]             128\n",
       "        ReLU: 3-3                    [10, 64, 128]             --\n",
       "        MaxPool1d: 3-4               [10, 64, 64]              --\n",
       "    Conv_block: 2-2                   [10, 128, 32]             --\n",
       "        Conv1d: 3-5                  [10, 128, 64]             24,576\n",
       "        BatchNorm1d: 3-6             [10, 128, 64]             256\n",
       "        ReLU: 3-7                    [10, 128, 64]             --\n",
       "        MaxPool1d: 3-8               [10, 128, 32]             --\n",
       "    Conv_block: 2-3                   [10, 256, 16]             --\n",
       "        Conv1d: 3-9                  [10, 256, 32]             98,304\n",
       "        BatchNorm1d: 3-10            [10, 256, 32]             512\n",
       "        ReLU: 3-11                   [10, 256, 32]             --\n",
       "        MaxPool1d: 3-12              [10, 256, 16]             --\n",
       "    Conv_block: 2-4                   [10, 512, 8]              --\n",
       "        Conv1d: 3-13                 [10, 512, 16]             393,216\n",
       "        BatchNorm1d: 3-14            [10, 512, 16]             1,024\n",
       "        ReLU: 3-15                   [10, 512, 16]             --\n",
       "        MaxPool1d: 3-16              [10, 512, 8]              --\n",
       "    Conv_block: 2-5                   [10, 512, 4]              --\n",
       "        Conv1d: 3-17                 [10, 512, 8]              786,432\n",
       "        BatchNorm1d: 3-18            [10, 512, 8]              1,024\n",
       "        ReLU: 3-19                   [10, 512, 8]              --\n",
       "        MaxPool1d: 3-20              [10, 512, 4]              --\n",
       "    Conv_block: 2-6                   [10, 512, 2]              --\n",
       "        Conv1d: 3-21                 [10, 512, 4]              786,432\n",
       "        BatchNorm1d: 3-22            [10, 512, 4]              1,024\n",
       "        ReLU: 3-23                   [10, 512, 4]              --\n",
       "        MaxPool1d: 3-24              [10, 512, 2]              --\n",
       "    Linear: 2-7                       [10, 48]                  49,200\n",
       "    Linear: 2-8                       [10, 1]                   49\n",
       "    Linear: 2-9                       [10, 1]                   1,025\n",
       "Spice_Decoder_lite: 1-2                [10, 32, 2]               --\n",
       "    Linear: 2-10                      [10, 1, 48]               96\n",
       "    Linear: 2-11                      [10, 1, 1024]             50,176\n",
       "    Deconv_block: 2-12                [10, 256, 2]              --\n",
       "        ConvTranspose1d: 3-25        [10, 256, 2]              393,216\n",
       "        BatchNorm1d: 3-26            [10, 256, 2]              512\n",
       "        ReLU: 3-27                   [10, 256, 2]              --\n",
       "    Deconv_block: 2-13                [10, 256, 2]              --\n",
       "        ConvTranspose1d: 3-28        [10, 256, 2]              196,608\n",
       "        BatchNorm1d: 3-29            [10, 256, 2]              512\n",
       "        ReLU: 3-30                   [10, 256, 2]              --\n",
       "    Deconv_block: 2-14                [10, 256, 2]              --\n",
       "        ConvTranspose1d: 3-31        [10, 256, 2]              196,608\n",
       "        BatchNorm1d: 3-32            [10, 256, 2]              512\n",
       "        ReLU: 3-33                   [10, 256, 2]              --\n",
       "    Deconv_block: 2-15                [10, 128, 2]              --\n",
       "        ConvTranspose1d: 3-34        [10, 128, 2]              98,304\n",
       "        BatchNorm1d: 3-35            [10, 128, 2]              256\n",
       "        ReLU: 3-36                   [10, 128, 2]              --\n",
       "    Deconv_block: 2-16                [10, 64, 2]               --\n",
       "        ConvTranspose1d: 3-37        [10, 64, 2]               24,576\n",
       "        BatchNorm1d: 3-38            [10, 64, 2]               128\n",
       "        ReLU: 3-39                   [10, 64, 2]               --\n",
       "    Deconv_block: 2-17                [10, 32, 2]               --\n",
       "        ConvTranspose1d: 3-40        [10, 32, 2]               6,144\n",
       "        BatchNorm1d: 3-41            [10, 32, 2]               64\n",
       "        ReLU: 3-42                   [10, 32, 2]               --\n",
       "==========================================================================================\n",
       "Total params: 3,111,106\n",
       "Trainable params: 3,111,106\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 224.09\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 6.63\n",
       "Params size (MB): 12.44\n",
       "Estimated Total Size (MB): 19.08\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = Spice_model2()\n",
    "summary(s, input_size=[10, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, param in s.named_parameters():\n",
    "    print(type(param), param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2fed21d7f443e9082cfc6ba2fbe018596127843c8658e141820a67f60820f8cf"
  },
  "kernelspec": {
   "display_name": "Python 3.10.11 ('mpa')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
