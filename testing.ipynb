{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-01 19:42:08.617772: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-01 19:42:08.656975: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-01 19:42:09.343492: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from utils.model import Spice_model\n",
    "from torchinfo import summary\n",
    "import pandas as pd\n",
    "from data_files.dataloader import MedleyDBLoader\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils.model import Spice_model, Spice_Encoder, Spice_Decoder\n",
    "from utils.model_types import Spice_Decoder_1Unpool, Spice_Decoder_1Unpool, Deconv_block\n",
    "from utils.training_script import Trainer\n",
    "from optims.loss import Huber_loss, Recons_loss, Conf_loss\n",
    "from data_files.dataset import CQT_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph torch_jit (\n",
      "  %input[FLOAT, batch_sizex128]\n",
      ") initializers (\n",
      "  %enc_block.fc1.weight[FLOAT, 48x1024]\n",
      "  %enc_block.fc1.bias[FLOAT, 48]\n",
      "  %enc_block.fc2.weight[FLOAT, 1x48]\n",
      "  %enc_block.fc2.bias[FLOAT, 1]\n",
      "  %enc_block.conf_head.weight[FLOAT, 1x1024]\n",
      "  %enc_block.conf_head.bias[FLOAT, 1]\n",
      "  %dec_block.deconv_block1.deconv.weight[FLOAT, 512x256x3]\n",
      "  %dec_block.deconv_block1.batchNorm.weight[FLOAT, 256]\n",
      "  %dec_block.deconv_block1.batchNorm.bias[FLOAT, 256]\n",
      "  %dec_block.deconv_block2.deconv.weight[FLOAT, 256x256x3]\n",
      "  %dec_block.deconv_block3.deconv.weight[FLOAT, 256x256x3]\n",
      "  %dec_block.deconv_block4.deconv.weight[FLOAT, 256x128x3]\n",
      "  %dec_block.deconv_block4.batchNorm.weight[FLOAT, 128]\n",
      "  %dec_block.deconv_block4.batchNorm.bias[FLOAT, 128]\n",
      "  %dec_block.deconv_block5.deconv.weight[FLOAT, 128x64x3]\n",
      "  %dec_block.deconv_block5.batchNorm.weight[FLOAT, 64]\n",
      "  %dec_block.deconv_block5.batchNorm.bias[FLOAT, 64]\n",
      "  %dec_block.deconv_block6.deconv.weight[FLOAT, 64x32x3]\n",
      "  %dec_block.deconv_block6.batchNorm.weight[FLOAT, 32]\n",
      "  %dec_block.deconv_block6.batchNorm.bias[FLOAT, 32]\n",
      "  %dec_block.fc1.bias[FLOAT, 48]\n",
      "  %dec_block.fc2.bias[FLOAT, 1024]\n",
      "  %onnx::Conv_154[FLOAT, 64x1x3]\n",
      "  %onnx::Conv_157[FLOAT, 128x64x3]\n",
      "  %onnx::Conv_160[FLOAT, 256x128x3]\n",
      "  %onnx::Conv_163[FLOAT, 512x256x3]\n",
      "  %onnx::Conv_164[FLOAT, 512]\n",
      "  %onnx::Conv_166[FLOAT, 512x512x3]\n",
      "  %onnx::Conv_169[FLOAT, 512x512x3]\n",
      "  %onnx::MatMul_172[FLOAT, 1x48]\n",
      "  %onnx::MatMul_173[FLOAT, 48x1024]\n",
      ") {\n",
      "  %onnx::Conv_170 = Identity(%onnx::Conv_164)\n",
      "  %onnx::Conv_167 = Identity(%onnx::Conv_164)\n",
      "  %onnx::Conv_161 = Identity(%dec_block.deconv_block1.batchNorm.bias)\n",
      "  %onnx::Conv_158 = Identity(%dec_block.deconv_block4.batchNorm.bias)\n",
      "  %onnx::Conv_155 = Identity(%dec_block.deconv_block5.batchNorm.bias)\n",
      "  %dec_block.deconv_block6.batchNorm.running_var = Identity(%dec_block.deconv_block6.batchNorm.weight)\n",
      "  %dec_block.deconv_block6.batchNorm.running_mean = Identity(%dec_block.deconv_block6.batchNorm.bias)\n",
      "  %dec_block.deconv_block5.batchNorm.running_var = Identity(%dec_block.deconv_block5.batchNorm.weight)\n",
      "  %dec_block.deconv_block5.batchNorm.running_mean = Identity(%dec_block.deconv_block5.batchNorm.bias)\n",
      "  %dec_block.deconv_block4.batchNorm.running_var = Identity(%dec_block.deconv_block4.batchNorm.weight)\n",
      "  %dec_block.deconv_block4.batchNorm.running_mean = Identity(%dec_block.deconv_block4.batchNorm.bias)\n",
      "  %dec_block.deconv_block3.batchNorm.running_var = Identity(%dec_block.deconv_block1.batchNorm.weight)\n",
      "  %dec_block.deconv_block3.batchNorm.running_mean = Identity(%dec_block.deconv_block1.batchNorm.bias)\n",
      "  %dec_block.deconv_block3.batchNorm.bias = Identity(%dec_block.deconv_block1.batchNorm.bias)\n",
      "  %dec_block.deconv_block3.batchNorm.weight = Identity(%dec_block.deconv_block1.batchNorm.weight)\n",
      "  %dec_block.deconv_block2.batchNorm.running_var = Identity(%dec_block.deconv_block1.batchNorm.weight)\n",
      "  %dec_block.deconv_block2.batchNorm.running_mean = Identity(%dec_block.deconv_block1.batchNorm.bias)\n",
      "  %dec_block.deconv_block2.batchNorm.bias = Identity(%dec_block.deconv_block1.batchNorm.bias)\n",
      "  %dec_block.deconv_block2.batchNorm.weight = Identity(%dec_block.deconv_block1.batchNorm.weight)\n",
      "  %dec_block.deconv_block1.batchNorm.running_var = Identity(%dec_block.deconv_block1.batchNorm.weight)\n",
      "  %dec_block.deconv_block1.batchNorm.running_mean = Identity(%dec_block.deconv_block1.batchNorm.bias)\n",
      "  %/enc_block/Unsqueeze_output_0 = Unsqueeze[axes = [1]](%input)\n",
      "  %/enc_block/Shape_output_0 = Shape(%/enc_block/Unsqueeze_output_0)\n",
      "  %/enc_block/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/enc_block/Gather_output_0 = Gather[axis = 0](%/enc_block/Shape_output_0, %/enc_block/Constant_output_0)\n",
      "  %/enc_block/conv_block1/conv/Conv_output_0 = Conv[dilations = [1], group = 1, kernel_shape = [3], pads = [1, 1], strides = [1]](%/enc_block/Unsqueeze_output_0, %onnx::Conv_154, %onnx::Conv_155)\n",
      "  %/enc_block/conv_block1/relu/Relu_output_0 = Relu(%/enc_block/conv_block1/conv/Conv_output_0)\n",
      "  %/enc_block/conv_block1/maxPool/MaxPool_output_0 = MaxPool[ceil_mode = 0, kernel_shape = [3], pads = [1, 1], strides = [2]](%/enc_block/conv_block1/relu/Relu_output_0)\n",
      "  %/enc_block/conv_block2/conv/Conv_output_0 = Conv[dilations = [1], group = 1, kernel_shape = [3], pads = [1, 1], strides = [1]](%/enc_block/conv_block1/maxPool/MaxPool_output_0, %onnx::Conv_157, %onnx::Conv_158)\n",
      "  %/enc_block/conv_block2/relu/Relu_output_0 = Relu(%/enc_block/conv_block2/conv/Conv_output_0)\n",
      "  %/enc_block/conv_block2/maxPool/MaxPool_output_0 = MaxPool[ceil_mode = 0, kernel_shape = [3], pads = [1, 1], strides = [2]](%/enc_block/conv_block2/relu/Relu_output_0)\n",
      "  %/enc_block/conv_block3/conv/Conv_output_0 = Conv[dilations = [1], group = 1, kernel_shape = [3], pads = [1, 1], strides = [1]](%/enc_block/conv_block2/maxPool/MaxPool_output_0, %onnx::Conv_160, %onnx::Conv_161)\n",
      "  %/enc_block/conv_block3/relu/Relu_output_0 = Relu(%/enc_block/conv_block3/conv/Conv_output_0)\n",
      "  %/enc_block/conv_block3/maxPool/MaxPool_output_0 = MaxPool[ceil_mode = 0, kernel_shape = [3], pads = [1, 1], strides = [2]](%/enc_block/conv_block3/relu/Relu_output_0)\n",
      "  %/enc_block/conv_block4/conv/Conv_output_0 = Conv[dilations = [1], group = 1, kernel_shape = [3], pads = [1, 1], strides = [1]](%/enc_block/conv_block3/maxPool/MaxPool_output_0, %onnx::Conv_163, %onnx::Conv_164)\n",
      "  %/enc_block/conv_block4/relu/Relu_output_0 = Relu(%/enc_block/conv_block4/conv/Conv_output_0)\n",
      "  %/enc_block/conv_block4/maxPool/MaxPool_output_0 = MaxPool[ceil_mode = 0, kernel_shape = [3], pads = [1, 1], strides = [2]](%/enc_block/conv_block4/relu/Relu_output_0)\n",
      "  %/enc_block/conv_block5/conv/Conv_output_0 = Conv[dilations = [1], group = 1, kernel_shape = [3], pads = [1, 1], strides = [1]](%/enc_block/conv_block4/maxPool/MaxPool_output_0, %onnx::Conv_166, %onnx::Conv_167)\n",
      "  %/enc_block/conv_block5/relu/Relu_output_0 = Relu(%/enc_block/conv_block5/conv/Conv_output_0)\n",
      "  %/enc_block/conv_block5/maxPool/MaxPool_output_0 = MaxPool[ceil_mode = 0, kernel_shape = [3], pads = [1, 1], strides = [2]](%/enc_block/conv_block5/relu/Relu_output_0)\n",
      "  %/enc_block/conv_block6/conv/Conv_output_0 = Conv[dilations = [1], group = 1, kernel_shape = [3], pads = [1, 1], strides = [1]](%/enc_block/conv_block5/maxPool/MaxPool_output_0, %onnx::Conv_169, %onnx::Conv_170)\n",
      "  %/enc_block/conv_block6/relu/Relu_output_0 = Relu(%/enc_block/conv_block6/conv/Conv_output_0)\n",
      "  %/enc_block/conv_block6/maxPool/MaxPool_output_0 = MaxPool[ceil_mode = 0, kernel_shape = [3], pads = [1, 1], strides = [2]](%/enc_block/conv_block6/relu/Relu_output_0)\n",
      "  %/enc_block/Unsqueeze_1_output_0 = Unsqueeze[axes = [0]](%/enc_block/Gather_output_0)\n",
      "  %/enc_block/Constant_1_output_0 = Constant[value = <Tensor>]()\n",
      "  %/enc_block/Concat_output_0 = Concat[axis = 0](%/enc_block/Unsqueeze_1_output_0, %/enc_block/Constant_1_output_0)\n",
      "  %/enc_block/Reshape_output_0 = Reshape(%/enc_block/conv_block6/maxPool/MaxPool_output_0, %/enc_block/Concat_output_0)\n",
      "  %/enc_block/fc1/Gemm_output_0 = Gemm[alpha = 1, beta = 1, transB = 1](%/enc_block/Reshape_output_0, %enc_block.fc1.weight, %enc_block.fc1.bias)\n",
      "  %output = Gemm[alpha = 1, beta = 1, transB = 1](%/enc_block/fc1/Gemm_output_0, %enc_block.fc2.weight, %enc_block.fc2.bias)\n",
      "  %118 = Gemm[alpha = 1, beta = 1, transB = 1](%/enc_block/Reshape_output_0, %enc_block.conf_head.weight, %enc_block.conf_head.bias)\n",
      "  %/dec_block/Unsqueeze_output_0 = Unsqueeze[axes = [1]](%output)\n",
      "  %/dec_block/Shape_output_0 = Shape(%/dec_block/Unsqueeze_output_0)\n",
      "  %/dec_block/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/dec_block/Gather_output_0 = Gather[axis = 0](%/dec_block/Shape_output_0, %/dec_block/Constant_output_0)\n",
      "  %/dec_block/fc1/MatMul_output_0 = MatMul(%/dec_block/Unsqueeze_output_0, %onnx::MatMul_172)\n",
      "  %/dec_block/fc1/Add_output_0 = Add(%dec_block.fc1.bias, %/dec_block/fc1/MatMul_output_0)\n",
      "  %/dec_block/fc2/MatMul_output_0 = MatMul(%/dec_block/fc1/Add_output_0, %onnx::MatMul_173)\n",
      "  %/dec_block/fc2/Add_output_0 = Add(%dec_block.fc2.bias, %/dec_block/fc2/MatMul_output_0)\n",
      "  %/dec_block/Unsqueeze_1_output_0 = Unsqueeze[axes = [0]](%/dec_block/Gather_output_0)\n",
      "  %/dec_block/Constant_1_output_0 = Constant[value = <Tensor>]()\n",
      "  %/dec_block/Constant_2_output_0 = Constant[value = <Tensor>]()\n",
      "  %/dec_block/Concat_output_0 = Concat[axis = 0](%/dec_block/Unsqueeze_1_output_0, %/dec_block/Constant_1_output_0, %/dec_block/Constant_2_output_0)\n",
      "  %/dec_block/Reshape_output_0 = Reshape(%/dec_block/fc2/Add_output_0, %/dec_block/Concat_output_0)\n",
      "  %/dec_block/deconv_block1/deconv/ConvTranspose_output_0 = ConvTranspose[dilations = [1], group = 1, kernel_shape = [3], pads = [1, 1], strides = [1]](%/dec_block/Reshape_output_0, %dec_block.deconv_block1.deconv.weight)\n",
      "  %/dec_block/deconv_block1/batchNorm/BatchNormalization_output_0 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%/dec_block/deconv_block1/deconv/ConvTranspose_output_0, %dec_block.deconv_block1.batchNorm.weight, %dec_block.deconv_block1.batchNorm.bias, %dec_block.deconv_block1.batchNorm.running_mean, %dec_block.deconv_block1.batchNorm.running_var)\n",
      "  %/dec_block/deconv_block1/relu/Relu_output_0 = Relu(%/dec_block/deconv_block1/batchNorm/BatchNormalization_output_0)\n",
      "  %/dec_block/deconv_block2/deconv/ConvTranspose_output_0 = ConvTranspose[dilations = [1], group = 1, kernel_shape = [3], pads = [1, 1], strides = [1]](%/dec_block/deconv_block1/relu/Relu_output_0, %dec_block.deconv_block2.deconv.weight)\n",
      "  %/dec_block/deconv_block2/batchNorm/BatchNormalization_output_0 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%/dec_block/deconv_block2/deconv/ConvTranspose_output_0, %dec_block.deconv_block2.batchNorm.weight, %dec_block.deconv_block2.batchNorm.bias, %dec_block.deconv_block2.batchNorm.running_mean, %dec_block.deconv_block2.batchNorm.running_var)\n",
      "  %/dec_block/deconv_block2/relu/Relu_output_0 = Relu(%/dec_block/deconv_block2/batchNorm/BatchNormalization_output_0)\n",
      "  %/dec_block/deconv_block3/deconv/ConvTranspose_output_0 = ConvTranspose[dilations = [1], group = 1, kernel_shape = [3], pads = [1, 1], strides = [1]](%/dec_block/deconv_block2/relu/Relu_output_0, %dec_block.deconv_block3.deconv.weight)\n",
      "  %/dec_block/deconv_block3/batchNorm/BatchNormalization_output_0 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%/dec_block/deconv_block3/deconv/ConvTranspose_output_0, %dec_block.deconv_block3.batchNorm.weight, %dec_block.deconv_block3.batchNorm.bias, %dec_block.deconv_block3.batchNorm.running_mean, %dec_block.deconv_block3.batchNorm.running_var)\n",
      "  %/dec_block/deconv_block3/relu/Relu_output_0 = Relu(%/dec_block/deconv_block3/batchNorm/BatchNormalization_output_0)\n",
      "  %/dec_block/deconv_block4/deconv/ConvTranspose_output_0 = ConvTranspose[dilations = [1], group = 1, kernel_shape = [3], pads = [1, 1], strides = [1]](%/dec_block/deconv_block3/relu/Relu_output_0, %dec_block.deconv_block4.deconv.weight)\n",
      "  %/dec_block/deconv_block4/batchNorm/BatchNormalization_output_0 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%/dec_block/deconv_block4/deconv/ConvTranspose_output_0, %dec_block.deconv_block4.batchNorm.weight, %dec_block.deconv_block4.batchNorm.bias, %dec_block.deconv_block4.batchNorm.running_mean, %dec_block.deconv_block4.batchNorm.running_var)\n",
      "  %/dec_block/deconv_block4/relu/Relu_output_0 = Relu(%/dec_block/deconv_block4/batchNorm/BatchNormalization_output_0)\n",
      "  %/dec_block/deconv_block5/deconv/ConvTranspose_output_0 = ConvTranspose[dilations = [1], group = 1, kernel_shape = [3], pads = [1, 1], strides = [1]](%/dec_block/deconv_block4/relu/Relu_output_0, %dec_block.deconv_block5.deconv.weight)\n",
      "  %/dec_block/deconv_block5/batchNorm/BatchNormalization_output_0 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%/dec_block/deconv_block5/deconv/ConvTranspose_output_0, %dec_block.deconv_block5.batchNorm.weight, %dec_block.deconv_block5.batchNorm.bias, %dec_block.deconv_block5.batchNorm.running_mean, %dec_block.deconv_block5.batchNorm.running_var)\n",
      "  %/dec_block/deconv_block5/relu/Relu_output_0 = Relu(%/dec_block/deconv_block5/batchNorm/BatchNormalization_output_0)\n",
      "  %/dec_block/deconv_block6/deconv/ConvTranspose_output_0 = ConvTranspose[dilations = [1], group = 1, kernel_shape = [3], pads = [1, 1], strides = [1]](%/dec_block/deconv_block5/relu/Relu_output_0, %dec_block.deconv_block6.deconv.weight)\n",
      "  %/dec_block/deconv_block6/batchNorm/BatchNormalization_output_0 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%/dec_block/deconv_block6/deconv/ConvTranspose_output_0, %dec_block.deconv_block6.batchNorm.weight, %dec_block.deconv_block6.batchNorm.bias, %dec_block.deconv_block6.batchNorm.running_mean, %dec_block.deconv_block6.batchNorm.running_var)\n",
      "  %152 = Relu(%/dec_block/deconv_block6/batchNorm/BatchNormalization_output_0)\n",
      "  return %output, %118, %152\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "# Load the ONNX model\n",
    "model = onnx.load(\"check.onnx\")\n",
    "\n",
    "# Check that the model is well formed\n",
    "onnx.checker.check_model(model)\n",
    "\n",
    "# Print a human readable representation of the graph\n",
    "print(onnx.helper.printable_graph(model.graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP Error /onnxruntime_src/onnxruntime/core/providers/cuda/cuda_call.cc:121 std::conditional_t<THRW, void, onnxruntime::common::Status> onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*, const char*, int) [with ERRTYPE = cudaError; bool THRW = true; std::conditional_t<THRW, void, onnxruntime::common::Status> = void] /onnxruntime_src/onnxruntime/core/providers/cuda/cuda_call.cc:114 std::conditional_t<THRW, void, onnxruntime::common::Status> onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*, const char*, int) [with ERRTYPE = cudaError; bool THRW = true; std::conditional_t<THRW, void, onnxruntime::common::Status> = void] CUDA failure 710: device-side assert triggered ; GPU=0 ; hostname=pop-os ; file=/onnxruntime_src/onnxruntime/core/providers/cuda/cuda_execution_provider.cc ; line=241 ; expr=cudaDeviceSynchronize(); \n",
      "\n",
      " when using ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "Falling back to ['CUDAExecutionProvider', 'CPUExecutionProvider'] and retrying.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-31 21:09:51.187206836 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:614 CreateExecutionProviderInstance] Failed to create TensorrtExecutionProvider. Please reference https://onnxruntime.ai/docs/execution-providers/TensorRT-ExecutionProvider.html#requirements to ensure all dependencies are met.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "/onnxruntime_src/onnxruntime/core/providers/cuda/cuda_call.cc:121 std::conditional_t<THRW, void, onnxruntime::common::Status> onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*, const char*, int) [with ERRTYPE = cudaError; bool THRW = true; std::conditional_t<THRW, void, onnxruntime::common::Status> = void] /onnxruntime_src/onnxruntime/core/providers/cuda/cuda_call.cc:114 std::conditional_t<THRW, void, onnxruntime::common::Status> onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*, const char*, int) [with ERRTYPE = cudaError; bool THRW = true; std::conditional_t<THRW, void, onnxruntime::common::Status> = void] CUDA failure 710: device-side assert triggered ; GPU=0 ; hostname=pop-os ; file=/onnxruntime_src/onnxruntime/core/providers/cuda/cuda_execution_provider.cc ; line=241 ; expr=cudaDeviceSynchronize(); \n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:383\u001b[0m, in \u001b[0;36mInferenceSession.__init__\u001b[0;34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 383\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_inference_session(providers, provider_options, disabled_optimizers)\n\u001b[1;32m    384\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mRuntimeError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:435\u001b[0m, in \u001b[0;36mInferenceSession._create_inference_session\u001b[0;34m(self, providers, provider_options, disabled_optimizers)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[39m# initialize the C++ InferenceSession\u001b[39;00m\n\u001b[0;32m--> 435\u001b[0m sess\u001b[39m.\u001b[39;49minitialize_session(providers, provider_options, disabled_optimizers)\n\u001b[1;32m    437\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sess \u001b[39m=\u001b[39m sess\n",
      "\u001b[0;31mRuntimeError\u001b[0m: /onnxruntime_src/onnxruntime/core/providers/cuda/cuda_call.cc:121 std::conditional_t<THRW, void, onnxruntime::common::Status> onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*, const char*, int) [with ERRTYPE = cudaError; bool THRW = true; std::conditional_t<THRW, void, onnxruntime::common::Status> = void] /onnxruntime_src/onnxruntime/core/providers/cuda/cuda_call.cc:114 std::conditional_t<THRW, void, onnxruntime::common::Status> onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*, const char*, int) [with ERRTYPE = cudaError; bool THRW = true; std::conditional_t<THRW, void, onnxruntime::common::Status> = void] CUDA failure 710: device-side assert triggered ; GPU=0 ; hostname=pop-os ; file=/onnxruntime_src/onnxruntime/core/providers/cuda/cuda_execution_provider.cc ; line=241 ; expr=cudaDeviceSynchronize(); \n\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39monnxruntime\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mort\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m ort_session \u001b[39m=\u001b[39m ort\u001b[39m.\u001b[39;49mInferenceSession(\u001b[39m\"\u001b[39;49m\u001b[39mcheck.onnx\u001b[39;49m\u001b[39m\"\u001b[39;49m, providers\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mTensorrtExecutionProvider\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mCUDAExecutionProvider\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mCPUExecutionProvider\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m      5\u001b[0m outputs \u001b[39m=\u001b[39m ort_session\u001b[39m.\u001b[39mrun(\n\u001b[1;32m      6\u001b[0m     \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mactual_input_1\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandn(\u001b[39m1\u001b[39m, \u001b[39m128\u001b[39m)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32)},\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(outputs[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:394\u001b[0m, in \u001b[0;36mInferenceSession.__init__\u001b[0;34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    393\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m fallback_error:\n\u001b[0;32m--> 394\u001b[0m         \u001b[39mraise\u001b[39;00m fallback_error \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[39m# Fallback is disabled. Raise the original error.\u001b[39;00m\n\u001b[1;32m    396\u001b[0m \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:389\u001b[0m, in \u001b[0;36mInferenceSession.__init__\u001b[0;34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEP Error \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m when using \u001b[39m\u001b[39m{\u001b[39;00mproviders\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    388\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFalling back to \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fallback_providers\u001b[39m}\u001b[39;00m\u001b[39m and retrying.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 389\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_inference_session(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fallback_providers, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    390\u001b[0m \u001b[39m# Fallback only once.\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisable_fallback()\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:435\u001b[0m, in \u001b[0;36mInferenceSession._create_inference_session\u001b[0;34m(self, providers, provider_options, disabled_optimizers)\u001b[0m\n\u001b[1;32m    432\u001b[0m     disabled_optimizers \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(disabled_optimizers)\n\u001b[1;32m    434\u001b[0m \u001b[39m# initialize the C++ InferenceSession\u001b[39;00m\n\u001b[0;32m--> 435\u001b[0m sess\u001b[39m.\u001b[39;49minitialize_session(providers, provider_options, disabled_optimizers)\n\u001b[1;32m    437\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sess \u001b[39m=\u001b[39m sess\n\u001b[1;32m    438\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sess_options \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sess\u001b[39m.\u001b[39msession_options\n",
      "\u001b[0;31mRuntimeError\u001b[0m: /onnxruntime_src/onnxruntime/core/providers/cuda/cuda_call.cc:121 std::conditional_t<THRW, void, onnxruntime::common::Status> onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*, const char*, int) [with ERRTYPE = cudaError; bool THRW = true; std::conditional_t<THRW, void, onnxruntime::common::Status> = void] /onnxruntime_src/onnxruntime/core/providers/cuda/cuda_call.cc:114 std::conditional_t<THRW, void, onnxruntime::common::Status> onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*, const char*, int) [with ERRTYPE = cudaError; bool THRW = true; std::conditional_t<THRW, void, onnxruntime::common::Status> = void] CUDA failure 710: device-side assert triggered ; GPU=0 ; hostname=pop-os ; file=/onnxruntime_src/onnxruntime/core/providers/cuda/cuda_execution_provider.cc ; line=241 ; expr=cudaDeviceSynchronize(); \n\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "ort_session = ort.InferenceSession(\"check.onnx\", providers=['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
    "\n",
    "outputs = ort_session.run(\n",
    "    None,\n",
    "    {\"actual_input_1\": np.random.randn(1, 128).astype(np.float32)},\n",
    ")\n",
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MaxUnpool1d.forward() missing 1 required positional argument: 'indices'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m mod \u001b[39m=\u001b[39m Deconv_block()\n\u001b[0;32m----> 2\u001b[0m ss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49mtrace(mod, torch\u001b[39m.\u001b[39;49mrand(\u001b[39m10\u001b[39;49m, \u001b[39m64\u001b[39;49m, \u001b[39m64\u001b[39;49m))\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/torch/jit/_trace.py:794\u001b[0m, in \u001b[0;36mtrace\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_kwarg_inputs, _store_inputs)\u001b[0m\n\u001b[1;32m    792\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    793\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mexample_kwarg_inputs should be a dict\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 794\u001b[0m     \u001b[39mreturn\u001b[39;00m trace_module(\n\u001b[1;32m    795\u001b[0m         func,\n\u001b[1;32m    796\u001b[0m         {\u001b[39m\"\u001b[39;49m\u001b[39mforward\u001b[39;49m\u001b[39m\"\u001b[39;49m: example_inputs},\n\u001b[1;32m    797\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    798\u001b[0m         check_trace,\n\u001b[1;32m    799\u001b[0m         wrap_check_inputs(check_inputs),\n\u001b[1;32m    800\u001b[0m         check_tolerance,\n\u001b[1;32m    801\u001b[0m         strict,\n\u001b[1;32m    802\u001b[0m         _force_outplace,\n\u001b[1;32m    803\u001b[0m         _module_class,\n\u001b[1;32m    804\u001b[0m         example_inputs_is_kwarg\u001b[39m=\u001b[39;49m\u001b[39misinstance\u001b[39;49m(example_kwarg_inputs, \u001b[39mdict\u001b[39;49m),\n\u001b[1;32m    805\u001b[0m         _store_inputs\u001b[39m=\u001b[39;49m_store_inputs\n\u001b[1;32m    806\u001b[0m     )\n\u001b[1;32m    807\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    808\u001b[0m     \u001b[39mhasattr\u001b[39m(func, \u001b[39m\"\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    809\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(func\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule)\n\u001b[1;32m    810\u001b[0m     \u001b[39mand\u001b[39;00m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mforward\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    811\u001b[0m ):\n\u001b[1;32m    812\u001b[0m     \u001b[39mif\u001b[39;00m example_inputs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/torch/jit/_trace.py:1056\u001b[0m, in \u001b[0;36mtrace_module\u001b[0;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_inputs_is_kwarg, _store_inputs)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1055\u001b[0m     example_inputs \u001b[39m=\u001b[39m make_tuple(example_inputs)\n\u001b[0;32m-> 1056\u001b[0m     module\u001b[39m.\u001b[39;49m_c\u001b[39m.\u001b[39;49m_create_method_from_trace(\n\u001b[1;32m   1057\u001b[0m         method_name,\n\u001b[1;32m   1058\u001b[0m         func,\n\u001b[1;32m   1059\u001b[0m         example_inputs,\n\u001b[1;32m   1060\u001b[0m         var_lookup_fn,\n\u001b[1;32m   1061\u001b[0m         strict,\n\u001b[1;32m   1062\u001b[0m         _force_outplace,\n\u001b[1;32m   1063\u001b[0m         argument_names,\n\u001b[1;32m   1064\u001b[0m         _store_inputs\n\u001b[1;32m   1065\u001b[0m     )\n\u001b[1;32m   1067\u001b[0m check_trace_method \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_c\u001b[39m.\u001b[39m_get_method(method_name)\n\u001b[1;32m   1069\u001b[0m \u001b[39m# Check the trace against new traces created from user-specified inputs\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/torch/nn/modules/module.py:1488\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1486\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1488\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1489\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1490\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m~/Documents/ai_fau_study/Sose23/MPA_project/Mini_SPICE/utils/modelExtra.py:23\u001b[0m, in \u001b[0;36mDeconv_block.forward\u001b[0;34m(self, input_1D)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_1D):\n\u001b[1;32m     21\u001b[0m     \n\u001b[1;32m     22\u001b[0m     \u001b[39m# Unpool\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     input_1D \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munpool(input_1D)\n\u001b[1;32m     24\u001b[0m     \u001b[39m# else:\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[39m#     print(\"no unpooling\", self.unPooling, unpool_mat is None)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[39m# Transpose conv\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     input_1D \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeconv(input_1D)\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/torch/nn/modules/module.py:1488\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1486\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1488\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1489\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1490\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "\u001b[0;31mTypeError\u001b[0m: MaxUnpool1d.forward() missing 1 required positional argument: 'indices'"
     ]
    }
   ],
   "source": [
    "mod = Deconv_block()\n",
    "ss = torch.jit.trace(mod, torch.rand(10, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    input_1D: Tensor) -> Tensor:\n",
      "  relu = self.relu\n",
      "  batchNorm = self.batchNorm\n",
      "  deconv = self.deconv\n",
      "  _0 = (batchNorm).forward((deconv).forward(input_1D, ), )\n",
      "  return (relu).forward(_0, )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ss.code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "torch.manual_seed(191009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def foo_sum(x,y):\n",
    "    return x+ y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCell(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyCell, self).__init__()\n",
    "        self.linear = torch.nn.Linear(4, 4)\n",
    "        \n",
    "\n",
    "    def forward(self, x, h):\n",
    "        new_h = torch.tanh(self.linear(x) + h)\n",
    "        return foo_sum(new_h, new_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecisionGate(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        if x.sum() > 0:\n",
    "            return x\n",
    "        else:\n",
    "            return -x\n",
    "\n",
    "class MyCell2(torch.nn.Module):\n",
    "    def __init__(self, dg):\n",
    "        super(MyCell2, self).__init__()\n",
    "        self.dg = dg\n",
    "        self.linear = torch.nn.Linear(4, 4)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        new_h = torch.tanh(self.dg(self.linear(x)) + h)\n",
    "        return new_h, new_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_cell = MyCell()\n",
    "x = torch.rand(3, 4)\n",
    "h = torch.rand(3, 4)\n",
    "print(my_cell(x, h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracd MyCell(\n",
      "  original_name=MyCell\n",
      "  (linear): Linear(original_name=Linear)\n",
      ") end\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7430,  1.4186,  1.5269,  0.7439],\n",
       "        [ 0.6736, -0.4245,  0.9019, -0.2510],\n",
       "        [ 1.0948,  0.4671,  0.8218,  0.3636]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_cell = MyCell()\n",
    "x, h = torch.rand(3, 4), torch.rand(3, 4)\n",
    "traced_cell = torch.jit.trace(my_cell, (x, h))\n",
    "print(\"tracd\",traced_cell,\"end\")\n",
    "traced_cell(x, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    x: Tensor,\n",
      "    h: Tensor) -> Tensor:\n",
      "  linear = self.linear\n",
      "  new_h = torch.tanh(torch.add((linear).forward(x, ), h))\n",
      "  return __torch__.foo_sum(new_h, new_h, )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print(traced_cell.graph)\n",
    "print(traced_cell.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_cell = MyCell2(MyDecisionGate())\n",
    "traced_cell = torch.jit.trace(my_cell, (x, h))\n",
    "\n",
    "print(traced_cell.dg.code)\n",
    "print(traced_cell.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripted_gate = torch.jit.script(MyDecisionGate())\n",
    "\n",
    "my_cell = MyCell2(scripted_gate)\n",
    "scripted_cell = torch.jit.script(my_cell)\n",
    "\n",
    "print(scripted_gate.code)\n",
    "print(scripted_cell.code)\n",
    "print(scripted_cell(x, h))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyRNNLoop(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyRNNLoop, self).__init__()\n",
    "        self.cell = torch.jit.script(MyCell2(scripted_gate))\n",
    "        self.unpool = nn.MaxPool1d(kernel_size=3, stride=1, padding=1)\n",
    "    \n",
    "    def forward(self, xs):\n",
    "        h, y = torch.zeros(3, 4), torch.zeros(3, 4)\n",
    "        for i in range(xs.size(0)):\n",
    "            y, h = self.unpool(xs[i], h)\n",
    "        return y, h\n",
    "\n",
    "rnn_loop = torch.jit.script( MyRNNLoop())\n",
    "print(rnn_loop.code)\n",
    "print(rnn_loop.cell.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrapRNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WrapRNN, self).__init__()\n",
    "        self.loop = torch.jit.script(MyRNNLoop())\n",
    "\n",
    "    def forward(self, xs):\n",
    "        y, h = self.loop(xs)\n",
    "        temp=0\n",
    "        for i in h:\n",
    "            temp+=y\n",
    "        return torch.relu(y)+temp\n",
    "\n",
    "traced = torch.jit.script(WrapRNN())\n",
    "print(traced.loop.cell.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class convBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.batch = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxPool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1, return_indices=True)\n",
    "    def forward(self, input_1D):\n",
    "        input_1D = self.conv(input_1D)\n",
    "        input_1D = self.relu(self.batch(input_1D))\n",
    "        input_1D, indx_mat = self.maxPool(input_1D)\n",
    "\n",
    "        return input_1D, indx_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class deconvBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.deconv = nn.ConvTranspose1d(64, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.batchNorm = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.unpool = nn.MaxUnpool1d(kernel_size=3, stride=1, padding=0)\n",
    "    \n",
    "    def forward(self, input_1D1, idmat):\n",
    "        input_1D = self.unpool(input_1D1, idmat)\n",
    "        input_1D = self.deconv(input_1D)\n",
    "        # batch norm\n",
    "        input_1D = self.batchNorm(input_1D)\n",
    "        # relu \n",
    "        input_1D = self.relu(input_1D)\n",
    "        return input_1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class modelFull(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bll = convBlock()\n",
    "        self.deconv = deconvBlock()\n",
    "    def forward(self, x):\n",
    "            xx, y = self.bll(x)\n",
    "            xz = self.deconv(xx, y)\n",
    "            return xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Spice_model2(\n",
       "  original_name=Spice_model2\n",
       "  (enc_block): Spice_Encoder(\n",
       "    original_name=Spice_Encoder\n",
       "    (conv_block1): Conv_block(\n",
       "      original_name=Conv_block\n",
       "      (conv): Conv1d(original_name=Conv1d)\n",
       "      (batch): BatchNorm1d(original_name=BatchNorm1d)\n",
       "      (relu): ReLU(original_name=ReLU)\n",
       "      (maxPool): MaxPool1d(original_name=MaxPool1d)\n",
       "    )\n",
       "    (conv_block2): Conv_block(\n",
       "      original_name=Conv_block\n",
       "      (conv): Conv1d(original_name=Conv1d)\n",
       "      (batch): BatchNorm1d(original_name=BatchNorm1d)\n",
       "      (relu): ReLU(original_name=ReLU)\n",
       "      (maxPool): MaxPool1d(original_name=MaxPool1d)\n",
       "    )\n",
       "    (conv_block3): Conv_block(\n",
       "      original_name=Conv_block\n",
       "      (conv): Conv1d(original_name=Conv1d)\n",
       "      (batch): BatchNorm1d(original_name=BatchNorm1d)\n",
       "      (relu): ReLU(original_name=ReLU)\n",
       "      (maxPool): MaxPool1d(original_name=MaxPool1d)\n",
       "    )\n",
       "    (conv_block4): Conv_block(\n",
       "      original_name=Conv_block\n",
       "      (conv): Conv1d(original_name=Conv1d)\n",
       "      (batch): BatchNorm1d(original_name=BatchNorm1d)\n",
       "      (relu): ReLU(original_name=ReLU)\n",
       "      (maxPool): MaxPool1d(original_name=MaxPool1d)\n",
       "    )\n",
       "    (conv_block5): Conv_block(\n",
       "      original_name=Conv_block\n",
       "      (conv): Conv1d(original_name=Conv1d)\n",
       "      (batch): BatchNorm1d(original_name=BatchNorm1d)\n",
       "      (relu): ReLU(original_name=ReLU)\n",
       "      (maxPool): MaxPool1d(original_name=MaxPool1d)\n",
       "    )\n",
       "    (conv_block6): Conv_block(\n",
       "      original_name=Conv_block\n",
       "      (conv): Conv1d(original_name=Conv1d)\n",
       "      (batch): BatchNorm1d(original_name=BatchNorm1d)\n",
       "      (relu): ReLU(original_name=ReLU)\n",
       "      (maxPool): MaxPool1d(original_name=MaxPool1d)\n",
       "    )\n",
       "    (fc1): Linear(original_name=Linear)\n",
       "    (fc2): Linear(original_name=Linear)\n",
       "    (conf_head): Linear(original_name=Linear)\n",
       "  )\n",
       "  (dec_block): Spice_Decoder_lite(\n",
       "    original_name=Spice_Decoder_lite\n",
       "    (deconv_block1): Deconv_block(\n",
       "      original_name=Deconv_block\n",
       "      (deconv): ConvTranspose1d(original_name=ConvTranspose1d)\n",
       "      (batchNorm): BatchNorm1d(original_name=BatchNorm1d)\n",
       "      (relu): ReLU(original_name=ReLU)\n",
       "      (unpool): MaxUnpool1d(original_name=MaxUnpool1d)\n",
       "    )\n",
       "    (deconv_block2): Deconv_block(\n",
       "      original_name=Deconv_block\n",
       "      (deconv): ConvTranspose1d(original_name=ConvTranspose1d)\n",
       "      (batchNorm): BatchNorm1d(original_name=BatchNorm1d)\n",
       "      (relu): ReLU(original_name=ReLU)\n",
       "      (unpool): MaxUnpool1d(original_name=MaxUnpool1d)\n",
       "    )\n",
       "    (deconv_block3): Deconv_block(\n",
       "      original_name=Deconv_block\n",
       "      (deconv): ConvTranspose1d(original_name=ConvTranspose1d)\n",
       "      (batchNorm): BatchNorm1d(original_name=BatchNorm1d)\n",
       "      (relu): ReLU(original_name=ReLU)\n",
       "      (unpool): MaxUnpool1d(original_name=MaxUnpool1d)\n",
       "    )\n",
       "    (deconv_block4): Deconv_block(\n",
       "      original_name=Deconv_block\n",
       "      (deconv): ConvTranspose1d(original_name=ConvTranspose1d)\n",
       "      (batchNorm): BatchNorm1d(original_name=BatchNorm1d)\n",
       "      (relu): ReLU(original_name=ReLU)\n",
       "      (unpool): MaxUnpool1d(original_name=MaxUnpool1d)\n",
       "    )\n",
       "    (deconv_block5): Deconv_block(\n",
       "      original_name=Deconv_block\n",
       "      (deconv): ConvTranspose1d(original_name=ConvTranspose1d)\n",
       "      (batchNorm): BatchNorm1d(original_name=BatchNorm1d)\n",
       "      (relu): ReLU(original_name=ReLU)\n",
       "      (unpool): MaxUnpool1d(original_name=MaxUnpool1d)\n",
       "    )\n",
       "    (deconv_block6): Deconv_block(\n",
       "      original_name=Deconv_block\n",
       "      (deconv): ConvTranspose1d(original_name=ConvTranspose1d)\n",
       "      (batchNorm): BatchNorm1d(original_name=BatchNorm1d)\n",
       "      (relu): ReLU(original_name=ReLU)\n",
       "      (unpool): MaxUnpool1d(original_name=MaxUnpool1d)\n",
       "    )\n",
       "    (fc1): Linear(original_name=Linear)\n",
       "    (fc2): Linear(original_name=Linear)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.jit.trace(Spice_model2(), torch.rand(1,  128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "modelFull                                [2, 32, 66]               --\n",
       "├─convBlock: 1-1                         [2, 64, 64]               --\n",
       "│    └─Conv1d: 2-1                       [2, 64, 128]              192\n",
       "│    └─BatchNorm1d: 2-2                  [2, 64, 128]              128\n",
       "│    └─ReLU: 2-3                         [2, 64, 128]              --\n",
       "│    └─MaxPool1d: 2-4                    [2, 64, 64]               --\n",
       "├─deconvBlock: 1-2                       [2, 32, 66]               --\n",
       "│    └─MaxUnpool1d: 2-5                  [2, 64, 66]               --\n",
       "│    └─ConvTranspose1d: 2-6              [2, 32, 66]               6,144\n",
       "│    └─BatchNorm1d: 2-7                  [2, 32, 66]               64\n",
       "│    └─ReLU: 2-8                         [2, 32, 66]               --\n",
       "==========================================================================================\n",
       "Total params: 6,528\n",
       "Trainable params: 6,528\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.86\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.33\n",
       "Params size (MB): 0.03\n",
       "Estimated Total Size (MB): 0.36\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = modelFull()\n",
    "summary(s, input_size=[2, 1, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.jit.script(modelFull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spice = Spice_Decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.jit.script(spice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 16000\n",
    "med = MedleyDBLoader(fs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9528461205301544e+16"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = 1110.0644999999954 \n",
    "f2 = 5.684341886080802e-14\n",
    "dive = f1/f2\n",
    "dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54.11642779108129"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log2(dive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pd = pd.read_pickle('./CQT_data/MIR1kfull2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnp = np.load('./CQT_data/MedleyDB.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmax = np.max(data_pd.iloc[:, -1])\n",
    "fmin = np.min(data_pd.iloc[:, -1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.0000488496063"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4     147.070550\n",
       "5     144.144107\n",
       "6     146.255372\n",
       "7     150.942966\n",
       "8     148.706805\n",
       "9     143.909850\n",
       "10    134.453924\n",
       "11    130.507561\n",
       "12    129.241802\n",
       "13    130.081145\n",
       "14    131.580764\n",
       "15    132.231130\n",
       "16    132.231130\n",
       "17    131.801133\n",
       "18    127.603321\n",
       "Name: 190, dtype: float64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pd.iloc[:15, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143.90985009261954"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pd.iloc[5, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>0.005217</td>\n",
       "      <td>0.001848</td>\n",
       "      <td>0.005943</td>\n",
       "      <td>0.004031</td>\n",
       "      <td>0.005480</td>\n",
       "      <td>0.014429</td>\n",
       "      <td>0.005135</td>\n",
       "      <td>0.004574</td>\n",
       "      <td>0.006104</td>\n",
       "      <td>0.011626</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013965</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>0.006434</td>\n",
       "      <td>0.012345</td>\n",
       "      <td>0.009101</td>\n",
       "      <td>0.006797</td>\n",
       "      <td>0.003267</td>\n",
       "      <td>0.002287</td>\n",
       "      <td>0.000724</td>\n",
       "      <td>2.842171e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>0.049793</td>\n",
       "      <td>0.055183</td>\n",
       "      <td>0.059535</td>\n",
       "      <td>0.067379</td>\n",
       "      <td>0.069461</td>\n",
       "      <td>0.072995</td>\n",
       "      <td>0.081257</td>\n",
       "      <td>0.084703</td>\n",
       "      <td>0.088940</td>\n",
       "      <td>0.089392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>0.000497</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>1.250555e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3349</th>\n",
       "      <td>0.017742</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>0.017584</td>\n",
       "      <td>0.017722</td>\n",
       "      <td>0.007697</td>\n",
       "      <td>0.003557</td>\n",
       "      <td>0.016785</td>\n",
       "      <td>0.013794</td>\n",
       "      <td>0.017591</td>\n",
       "      <td>0.016116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020716</td>\n",
       "      <td>0.019512</td>\n",
       "      <td>0.026083</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.035206</td>\n",
       "      <td>0.015362</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>0.013085</td>\n",
       "      <td>0.007575</td>\n",
       "      <td>2.842171e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7113</th>\n",
       "      <td>0.002018</td>\n",
       "      <td>0.038668</td>\n",
       "      <td>0.054554</td>\n",
       "      <td>0.031007</td>\n",
       "      <td>0.028359</td>\n",
       "      <td>0.011903</td>\n",
       "      <td>0.013359</td>\n",
       "      <td>0.050509</td>\n",
       "      <td>0.047573</td>\n",
       "      <td>0.019750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000478</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>0.000314</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>2.842171e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7965</th>\n",
       "      <td>0.037131</td>\n",
       "      <td>0.035321</td>\n",
       "      <td>0.031628</td>\n",
       "      <td>0.027804</td>\n",
       "      <td>0.035301</td>\n",
       "      <td>0.043344</td>\n",
       "      <td>0.032813</td>\n",
       "      <td>0.031208</td>\n",
       "      <td>0.029053</td>\n",
       "      <td>0.027769</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005665</td>\n",
       "      <td>0.007035</td>\n",
       "      <td>0.004666</td>\n",
       "      <td>0.006148</td>\n",
       "      <td>0.003617</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.000283</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.000498</td>\n",
       "      <td>1.893487e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240573</th>\n",
       "      <td>0.005941</td>\n",
       "      <td>0.004063</td>\n",
       "      <td>0.004148</td>\n",
       "      <td>0.006147</td>\n",
       "      <td>0.005780</td>\n",
       "      <td>0.003504</td>\n",
       "      <td>0.002276</td>\n",
       "      <td>0.002994</td>\n",
       "      <td>0.004917</td>\n",
       "      <td>0.005770</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000430</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>9.123369e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249465</th>\n",
       "      <td>0.007889</td>\n",
       "      <td>0.006155</td>\n",
       "      <td>0.011544</td>\n",
       "      <td>0.014008</td>\n",
       "      <td>0.002876</td>\n",
       "      <td>0.009931</td>\n",
       "      <td>0.015477</td>\n",
       "      <td>0.008524</td>\n",
       "      <td>0.009942</td>\n",
       "      <td>0.010375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>1.776357e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250009</th>\n",
       "      <td>0.017629</td>\n",
       "      <td>0.026075</td>\n",
       "      <td>0.028820</td>\n",
       "      <td>0.017638</td>\n",
       "      <td>0.003201</td>\n",
       "      <td>0.016272</td>\n",
       "      <td>0.011814</td>\n",
       "      <td>0.003255</td>\n",
       "      <td>0.014340</td>\n",
       "      <td>0.034213</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>2.060574e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250766</th>\n",
       "      <td>0.089455</td>\n",
       "      <td>0.081540</td>\n",
       "      <td>0.073750</td>\n",
       "      <td>0.095536</td>\n",
       "      <td>0.104619</td>\n",
       "      <td>0.096023</td>\n",
       "      <td>0.114201</td>\n",
       "      <td>0.116220</td>\n",
       "      <td>0.100998</td>\n",
       "      <td>0.105345</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.828575e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251012</th>\n",
       "      <td>0.007578</td>\n",
       "      <td>0.001632</td>\n",
       "      <td>0.006024</td>\n",
       "      <td>0.005676</td>\n",
       "      <td>0.018743</td>\n",
       "      <td>0.008673</td>\n",
       "      <td>0.009874</td>\n",
       "      <td>0.009045</td>\n",
       "      <td>0.011752</td>\n",
       "      <td>0.013168</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>1.963195e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>161 rows × 191 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6    \\\n",
       "344     0.005217  0.001848  0.005943  0.004031  0.005480  0.014429  0.005135   \n",
       "670     0.049793  0.055183  0.059535  0.067379  0.069461  0.072995  0.081257   \n",
       "3349    0.017742  0.000813  0.017584  0.017722  0.007697  0.003557  0.016785   \n",
       "7113    0.002018  0.038668  0.054554  0.031007  0.028359  0.011903  0.013359   \n",
       "7965    0.037131  0.035321  0.031628  0.027804  0.035301  0.043344  0.032813   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "240573  0.005941  0.004063  0.004148  0.006147  0.005780  0.003504  0.002276   \n",
       "249465  0.007889  0.006155  0.011544  0.014008  0.002876  0.009931  0.015477   \n",
       "250009  0.017629  0.026075  0.028820  0.017638  0.003201  0.016272  0.011814   \n",
       "250766  0.089455  0.081540  0.073750  0.095536  0.104619  0.096023  0.114201   \n",
       "251012  0.007578  0.001632  0.006024  0.005676  0.018743  0.008673  0.009874   \n",
       "\n",
       "             7         8         9    ...       181       182       183  \\\n",
       "344     0.004574  0.006104  0.011626  ...  0.013965  0.008797  0.006434   \n",
       "670     0.084703  0.088940  0.089392  ...  0.000111  0.000414  0.000422   \n",
       "3349    0.013794  0.017591  0.016116  ...  0.020716  0.019512  0.026083   \n",
       "7113    0.050509  0.047573  0.019750  ...  0.000478  0.000263  0.000362   \n",
       "7965    0.031208  0.029053  0.027769  ...  0.005665  0.007035  0.004666   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "240573  0.002994  0.004917  0.005770  ...  0.000430  0.000277  0.000088   \n",
       "249465  0.008524  0.009942  0.010375  ...  0.000177  0.000099  0.000161   \n",
       "250009  0.003255  0.014340  0.034213  ...  0.000214  0.000032  0.000142   \n",
       "250766  0.116220  0.100998  0.105345  ...  0.000017  0.000022  0.000046   \n",
       "251012  0.009045  0.011752  0.013168  ...  0.000041  0.000068  0.000049   \n",
       "\n",
       "             184       185       186       187       188       189  \\\n",
       "344     0.012345  0.009101  0.006797  0.003267  0.002287  0.000724   \n",
       "670     0.000497  0.000529  0.000217  0.000247  0.000072  0.000217   \n",
       "3349    0.033203  0.035206  0.015362  0.012048  0.013085  0.007575   \n",
       "7113    0.000425  0.000416  0.000314  0.000255  0.000276  0.000321   \n",
       "7965    0.006148  0.003617  0.000516  0.000283  0.000170  0.000498   \n",
       "...          ...       ...       ...       ...       ...       ...   \n",
       "240573  0.000139  0.000028  0.000089  0.000079  0.000065  0.000072   \n",
       "249465  0.000101  0.000135  0.000128  0.000038  0.000027  0.000044   \n",
       "250009  0.000151  0.000130  0.000080  0.000051  0.000044  0.000054   \n",
       "250766  0.000074  0.000073  0.000109  0.000094  0.000068  0.000100   \n",
       "251012  0.000054  0.000079  0.000061  0.000085  0.000050  0.000026   \n",
       "\n",
       "                 190  \n",
       "344     2.842171e-14  \n",
       "670     1.250555e-11  \n",
       "3349    2.842171e-14  \n",
       "7113    2.842171e-14  \n",
       "7965    1.893487e+01  \n",
       "...              ...  \n",
       "240573  9.123369e-12  \n",
       "249465  1.776357e-11  \n",
       "250009  2.060574e-11  \n",
       "250766  1.828575e+01  \n",
       "251012  1.963195e+01  \n",
       "\n",
       "[161 rows x 191 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pd.loc[data_pd.iloc[:, -1]<20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_pd = pd.DataFrame(data=nnp) \n",
    "train, val = train_test_split(data_pd, train_size=0.8, test_size=0.2, random_state=1)\n",
    "print(\"train shape: \", train.shape)\n",
    "train_batches = DataLoader(CQT_Dataset(data=train, mode='train'), batch_size=64, shuffle=True)\n",
    "print(\"train_batch NUmber: \", len(train_batches))\n",
    "diff, slice1, slice2, f0 = next(iter(train_batches))\n",
    "print(f\"diff batch shape: {diff.size()}\")\n",
    "print(f\"slice1 batch shape: {slice1.size()}\")\n",
    "print(f\"slice2 batch shape: {f0.size()}\")\n",
    "\n",
    "spice = Spice_model()\n",
    "\n",
    "\n",
    "for b in train_batches:\n",
    "    print(\"batch shape:\", b[0].shape)\n",
    "    pitch_diff, x_1, x_2, f0 = b\n",
    "    x_1 = x_1.type(torch.FloatTensor)\n",
    "    #print(x_1.shape, x_1.type())\n",
    "    a, x, y = spice(x_1)\n",
    "    print(a.size(), x.size(), y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx=torch.arange(0, 25, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = xx.reshape((5,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(358919, 191)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('./CQT_data/MDBSynth.pkl')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(181432, 191)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove rows of cqt where label (last) column is zero\n",
    "df.drop(df.loc[df.iloc[:, -1]<=0].index, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(181432, 192)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=1, random_state=10).reset_index()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(181432, 191)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=['index'],axis=1, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1205.3295446000004 -5.684341886080802e-14\n"
     ]
    }
   ],
   "source": [
    "fmax = np.max(df.iloc[:, -1])\n",
    "fmin = np.min(df.iloc[:, -1]) \n",
    "print(fmax, fmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.093583</td>\n",
       "      <td>0.103289</td>\n",
       "      <td>0.108589</td>\n",
       "      <td>0.114461</td>\n",
       "      <td>0.126163</td>\n",
       "      <td>0.141803</td>\n",
       "      <td>0.163479</td>\n",
       "      <td>0.186689</td>\n",
       "      <td>0.207631</td>\n",
       "      <td>0.221017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003004</td>\n",
       "      <td>0.001498</td>\n",
       "      <td>0.002785</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.002594</td>\n",
       "      <td>0.003086</td>\n",
       "      <td>0.003188</td>\n",
       "      <td>0.001229</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>66.010860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008210</td>\n",
       "      <td>0.008015</td>\n",
       "      <td>0.005654</td>\n",
       "      <td>0.004071</td>\n",
       "      <td>0.002212</td>\n",
       "      <td>0.002080</td>\n",
       "      <td>0.001428</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>309.451418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.000325</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003706</td>\n",
       "      <td>0.005361</td>\n",
       "      <td>0.010392</td>\n",
       "      <td>0.009005</td>\n",
       "      <td>0.004797</td>\n",
       "      <td>0.001981</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>164.620700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000322</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>299.930182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000541</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.001701</td>\n",
       "      <td>0.001012</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>123.772538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 191 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.093583  0.103289  0.108589  0.114461  0.126163  0.141803  0.163479   \n",
       "1  0.000082  0.000233  0.000090  0.000190  0.000066  0.000029  0.000197   \n",
       "2  0.000258  0.000223  0.000260  0.000323  0.000325  0.000267  0.000267   \n",
       "3  0.000109  0.000086  0.000039  0.000051  0.000037  0.000029  0.000061   \n",
       "4  0.000092  0.000159  0.000204  0.000082  0.000007  0.000048  0.000038   \n",
       "\n",
       "        7         8         9    ...       181       182       183       184  \\\n",
       "0  0.186689  0.207631  0.221017  ...  0.003004  0.001498  0.002785  0.003538   \n",
       "1  0.000035  0.000137  0.000152  ...  0.008210  0.008015  0.005654  0.004071   \n",
       "2  0.000327  0.000348  0.000338  ...  0.003706  0.005361  0.010392  0.009005   \n",
       "3  0.000103  0.000143  0.000165  ...  0.000322  0.000238  0.000251  0.000221   \n",
       "4  0.000063  0.000103  0.000153  ...  0.000320  0.000105  0.000541  0.001700   \n",
       "\n",
       "        185       186       187       188       189         190  \n",
       "0  0.002594  0.003086  0.003188  0.001229  0.000061   66.010860  \n",
       "1  0.002212  0.002080  0.001428  0.000503  0.000096  309.451418  \n",
       "2  0.004797  0.001981  0.000298  0.000261  0.000129  164.620700  \n",
       "3  0.000133  0.000084  0.000069  0.000024  0.000017  299.930182  \n",
       "4  0.001701  0.001012  0.000401  0.000046  0.000040  123.772538  \n",
       "\n",
       "[5 rows x 191 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(0,25,1).reshape(5,5)[:,-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  1.  ,  2.  , -0.25],\n",
       "       [ 3.  ,  4.  ,  5.  ,  0.7 ],\n",
       "       [ 6.  ,  7.  ,  8.  ,  0.  ],\n",
       "       [ 9.  , 10.  , 11.  , -0.5 ],\n",
       "       [12.  , 13.  , 14.  ,  0.  ]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(0,15,1).reshape(5,3)\n",
    "a = np.hstack((a, np.array([-0.25,0.7,0,-0.5,0]).reshape(-1,1)))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 4)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(a)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('index',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2     3\n",
       "0   6.0   7.0   8.0  0.00\n",
       "1   9.0  10.0  11.0 -0.50\n",
       "2   0.0   1.0   2.0 -0.25\n",
       "3  12.0  13.0  14.0  0.00\n",
       "4   3.0   4.0   5.0  0.70"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(df.iloc[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,0,0,3,4,0,0,5,4,6,0,2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a[a!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 4, 6, 2, 1])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "isIntList() INTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/core/ivalue_inl.h\":1938, please report a bug to PyTorch. Expected IntList but got Int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49mscript(Spice_model2())\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/torch/jit/_script.py:1284\u001b[0m, in \u001b[0;36mscript\u001b[0;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule):\n\u001b[1;32m   1283\u001b[0m     obj \u001b[39m=\u001b[39m call_prepare_scriptable_func(obj)\n\u001b[0;32m-> 1284\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39m_recursive\u001b[39m.\u001b[39mcreate_script_module(\n\u001b[1;32m   1285\u001b[0m         obj, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39m_recursive\u001b[39m.\u001b[39minfer_methods_to_compile\n\u001b[1;32m   1286\u001b[0m     )\n\u001b[1;32m   1288\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, \u001b[39mdict\u001b[39m):\n\u001b[1;32m   1289\u001b[0m     \u001b[39mreturn\u001b[39;00m create_script_dict(obj)\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/torch/jit/_recursive.py:480\u001b[0m, in \u001b[0;36mcreate_script_module\u001b[0;34m(nn_module, stubs_fn, share_types, is_tracing)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tracing:\n\u001b[1;32m    479\u001b[0m     AttributeTypeIsSupportedChecker()\u001b[39m.\u001b[39mcheck(nn_module)\n\u001b[0;32m--> 480\u001b[0m \u001b[39mreturn\u001b[39;00m create_script_module_impl(nn_module, concrete_type, stubs_fn)\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/torch/jit/_recursive.py:542\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    539\u001b[0m     script_module\u001b[39m.\u001b[39m_concrete_type \u001b[39m=\u001b[39m concrete_type\n\u001b[1;32m    541\u001b[0m \u001b[39m# Actually create the ScriptModule, initializing it with the function we just defined\u001b[39;00m\n\u001b[0;32m--> 542\u001b[0m script_module \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49mRecursiveScriptModule\u001b[39m.\u001b[39;49m_construct(cpp_module, init_fn)\n\u001b[1;32m    544\u001b[0m \u001b[39m# Compile methods if necessary\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[39mif\u001b[39;00m concrete_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m concrete_type_store\u001b[39m.\u001b[39mmethods_compiled:\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/torch/jit/_script.py:614\u001b[0m, in \u001b[0;36mRecursiveScriptModule._construct\u001b[0;34m(cpp_module, init_fn)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    602\u001b[0m \u001b[39mConstruct a RecursiveScriptModule that's ready for use. PyTorch\u001b[39;00m\n\u001b[1;32m    603\u001b[0m \u001b[39mcode should use this to construct a RecursiveScriptModule instead\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[39m    init_fn:  Lambda that initializes the RecursiveScriptModule passed to it.\u001b[39;00m\n\u001b[1;32m    612\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    613\u001b[0m script_module \u001b[39m=\u001b[39m RecursiveScriptModule(cpp_module)\n\u001b[0;32m--> 614\u001b[0m init_fn(script_module)\n\u001b[1;32m    616\u001b[0m \u001b[39m# Finalize the ScriptModule: replace the nn.Module state with our\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[39m# custom implementations and flip the _initializing bit.\u001b[39;00m\n\u001b[1;32m    618\u001b[0m RecursiveScriptModule\u001b[39m.\u001b[39m_finalize_scriptmodule(script_module)\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/torch/jit/_recursive.py:520\u001b[0m, in \u001b[0;36mcreate_script_module_impl.<locals>.init_fn\u001b[0;34m(script_module)\u001b[0m\n\u001b[1;32m    517\u001b[0m     scripted \u001b[39m=\u001b[39m orig_value\n\u001b[1;32m    518\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     \u001b[39m# always reuse the provided stubs_fn to infer the methods to compile\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m     scripted \u001b[39m=\u001b[39m create_script_module_impl(orig_value, sub_concrete_type, stubs_fn)\n\u001b[1;32m    522\u001b[0m cpp_module\u001b[39m.\u001b[39msetattr(name, scripted)\n\u001b[1;32m    523\u001b[0m script_module\u001b[39m.\u001b[39m_modules[name] \u001b[39m=\u001b[39m scripted\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/torch/jit/_recursive.py:542\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    539\u001b[0m     script_module\u001b[39m.\u001b[39m_concrete_type \u001b[39m=\u001b[39m concrete_type\n\u001b[1;32m    541\u001b[0m \u001b[39m# Actually create the ScriptModule, initializing it with the function we just defined\u001b[39;00m\n\u001b[0;32m--> 542\u001b[0m script_module \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49mRecursiveScriptModule\u001b[39m.\u001b[39;49m_construct(cpp_module, init_fn)\n\u001b[1;32m    544\u001b[0m \u001b[39m# Compile methods if necessary\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[39mif\u001b[39;00m concrete_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m concrete_type_store\u001b[39m.\u001b[39mmethods_compiled:\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/torch/jit/_script.py:614\u001b[0m, in \u001b[0;36mRecursiveScriptModule._construct\u001b[0;34m(cpp_module, init_fn)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    602\u001b[0m \u001b[39mConstruct a RecursiveScriptModule that's ready for use. PyTorch\u001b[39;00m\n\u001b[1;32m    603\u001b[0m \u001b[39mcode should use this to construct a RecursiveScriptModule instead\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[39m    init_fn:  Lambda that initializes the RecursiveScriptModule passed to it.\u001b[39;00m\n\u001b[1;32m    612\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    613\u001b[0m script_module \u001b[39m=\u001b[39m RecursiveScriptModule(cpp_module)\n\u001b[0;32m--> 614\u001b[0m init_fn(script_module)\n\u001b[1;32m    616\u001b[0m \u001b[39m# Finalize the ScriptModule: replace the nn.Module state with our\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[39m# custom implementations and flip the _initializing bit.\u001b[39;00m\n\u001b[1;32m    618\u001b[0m RecursiveScriptModule\u001b[39m.\u001b[39m_finalize_scriptmodule(script_module)\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/torch/jit/_recursive.py:520\u001b[0m, in \u001b[0;36mcreate_script_module_impl.<locals>.init_fn\u001b[0;34m(script_module)\u001b[0m\n\u001b[1;32m    517\u001b[0m     scripted \u001b[39m=\u001b[39m orig_value\n\u001b[1;32m    518\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     \u001b[39m# always reuse the provided stubs_fn to infer the methods to compile\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m     scripted \u001b[39m=\u001b[39m create_script_module_impl(orig_value, sub_concrete_type, stubs_fn)\n\u001b[1;32m    522\u001b[0m cpp_module\u001b[39m.\u001b[39msetattr(name, scripted)\n\u001b[1;32m    523\u001b[0m script_module\u001b[39m.\u001b[39m_modules[name] \u001b[39m=\u001b[39m scripted\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/torch/jit/_recursive.py:542\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    539\u001b[0m     script_module\u001b[39m.\u001b[39m_concrete_type \u001b[39m=\u001b[39m concrete_type\n\u001b[1;32m    541\u001b[0m \u001b[39m# Actually create the ScriptModule, initializing it with the function we just defined\u001b[39;00m\n\u001b[0;32m--> 542\u001b[0m script_module \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49mRecursiveScriptModule\u001b[39m.\u001b[39;49m_construct(cpp_module, init_fn)\n\u001b[1;32m    544\u001b[0m \u001b[39m# Compile methods if necessary\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[39mif\u001b[39;00m concrete_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m concrete_type_store\u001b[39m.\u001b[39mmethods_compiled:\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/torch/jit/_script.py:614\u001b[0m, in \u001b[0;36mRecursiveScriptModule._construct\u001b[0;34m(cpp_module, init_fn)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    602\u001b[0m \u001b[39mConstruct a RecursiveScriptModule that's ready for use. PyTorch\u001b[39;00m\n\u001b[1;32m    603\u001b[0m \u001b[39mcode should use this to construct a RecursiveScriptModule instead\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[39m    init_fn:  Lambda that initializes the RecursiveScriptModule passed to it.\u001b[39;00m\n\u001b[1;32m    612\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    613\u001b[0m script_module \u001b[39m=\u001b[39m RecursiveScriptModule(cpp_module)\n\u001b[0;32m--> 614\u001b[0m init_fn(script_module)\n\u001b[1;32m    616\u001b[0m \u001b[39m# Finalize the ScriptModule: replace the nn.Module state with our\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[39m# custom implementations and flip the _initializing bit.\u001b[39;00m\n\u001b[1;32m    618\u001b[0m RecursiveScriptModule\u001b[39m.\u001b[39m_finalize_scriptmodule(script_module)\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/torch/jit/_recursive.py:520\u001b[0m, in \u001b[0;36mcreate_script_module_impl.<locals>.init_fn\u001b[0;34m(script_module)\u001b[0m\n\u001b[1;32m    517\u001b[0m     scripted \u001b[39m=\u001b[39m orig_value\n\u001b[1;32m    518\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     \u001b[39m# always reuse the provided stubs_fn to infer the methods to compile\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m     scripted \u001b[39m=\u001b[39m create_script_module_impl(orig_value, sub_concrete_type, stubs_fn)\n\u001b[1;32m    522\u001b[0m cpp_module\u001b[39m.\u001b[39msetattr(name, scripted)\n\u001b[1;32m    523\u001b[0m script_module\u001b[39m.\u001b[39m_modules[name] \u001b[39m=\u001b[39m scripted\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/torch/jit/_recursive.py:546\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[39m# Compile methods if necessary\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[39mif\u001b[39;00m concrete_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m concrete_type_store\u001b[39m.\u001b[39mmethods_compiled:\n\u001b[0;32m--> 546\u001b[0m     create_methods_and_properties_from_stubs(concrete_type, method_stubs, property_stubs)\n\u001b[1;32m    547\u001b[0m     \u001b[39m# Create hooks after methods to ensure no name collisions between hooks and methods.\u001b[39;00m\n\u001b[1;32m    548\u001b[0m     \u001b[39m# If done before, hooks can overshadow methods that aren't exported.\u001b[39;00m\n\u001b[1;32m    549\u001b[0m     create_hooks_from_stubs(concrete_type, hook_stubs, pre_hook_stubs)\n",
      "File \u001b[0;32m~/anaconda3/envs/mpa/lib/python3.10/site-packages/torch/jit/_recursive.py:397\u001b[0m, in \u001b[0;36mcreate_methods_and_properties_from_stubs\u001b[0;34m(concrete_type, method_stubs, property_stubs)\u001b[0m\n\u001b[1;32m    394\u001b[0m property_defs \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mdef_ \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m property_stubs]\n\u001b[1;32m    395\u001b[0m property_rcbs \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mresolution_callback \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m property_stubs]\n\u001b[0;32m--> 397\u001b[0m concrete_type\u001b[39m.\u001b[39;49m_create_methods_and_properties(property_defs, property_rcbs, method_defs, method_rcbs, method_defaults)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: isIntList() INTERNAL ASSERT FAILED at \"/opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/core/ivalue_inl.h\":1938, please report a bug to PyTorch. Expected IntList but got Int"
     ]
    }
   ],
   "source": [
    "torch.jit.script(Spice_model2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Spice_model2                             [10, 1]                   --\n",
       "├─Spice_Encoder: 1-1                     [10, 1]                   --\n",
       "│    └─Conv_block: 2-1                   [10, 64, 64]              --\n",
       "│    │    └─Conv1d: 3-1                  [10, 64, 128]             192\n",
       "│    │    └─BatchNorm1d: 3-2             [10, 64, 128]             128\n",
       "│    │    └─ReLU: 3-3                    [10, 64, 128]             --\n",
       "│    │    └─MaxPool1d: 3-4               [10, 64, 64]              --\n",
       "│    └─Conv_block: 2-2                   [10, 128, 32]             --\n",
       "│    │    └─Conv1d: 3-5                  [10, 128, 64]             24,576\n",
       "│    │    └─BatchNorm1d: 3-6             [10, 128, 64]             256\n",
       "│    │    └─ReLU: 3-7                    [10, 128, 64]             --\n",
       "│    │    └─MaxPool1d: 3-8               [10, 128, 32]             --\n",
       "│    └─Conv_block: 2-3                   [10, 256, 16]             --\n",
       "│    │    └─Conv1d: 3-9                  [10, 256, 32]             98,304\n",
       "│    │    └─BatchNorm1d: 3-10            [10, 256, 32]             512\n",
       "│    │    └─ReLU: 3-11                   [10, 256, 32]             --\n",
       "│    │    └─MaxPool1d: 3-12              [10, 256, 16]             --\n",
       "│    └─Conv_block: 2-4                   [10, 512, 8]              --\n",
       "│    │    └─Conv1d: 3-13                 [10, 512, 16]             393,216\n",
       "│    │    └─BatchNorm1d: 3-14            [10, 512, 16]             1,024\n",
       "│    │    └─ReLU: 3-15                   [10, 512, 16]             --\n",
       "│    │    └─MaxPool1d: 3-16              [10, 512, 8]              --\n",
       "│    └─Conv_block: 2-5                   [10, 512, 4]              --\n",
       "│    │    └─Conv1d: 3-17                 [10, 512, 8]              786,432\n",
       "│    │    └─BatchNorm1d: 3-18            [10, 512, 8]              1,024\n",
       "│    │    └─ReLU: 3-19                   [10, 512, 8]              --\n",
       "│    │    └─MaxPool1d: 3-20              [10, 512, 4]              --\n",
       "│    └─Conv_block: 2-6                   [10, 512, 2]              --\n",
       "│    │    └─Conv1d: 3-21                 [10, 512, 4]              786,432\n",
       "│    │    └─BatchNorm1d: 3-22            [10, 512, 4]              1,024\n",
       "│    │    └─ReLU: 3-23                   [10, 512, 4]              --\n",
       "│    │    └─MaxPool1d: 3-24              [10, 512, 2]              --\n",
       "│    └─Linear: 2-7                       [10, 48]                  49,200\n",
       "│    └─Linear: 2-8                       [10, 1]                   49\n",
       "│    └─Linear: 2-9                       [10, 1]                   1,025\n",
       "├─Spice_Decoder_lite: 1-2                [10, 32, 2]               --\n",
       "│    └─Linear: 2-10                      [10, 1, 48]               96\n",
       "│    └─Linear: 2-11                      [10, 1, 1024]             50,176\n",
       "│    └─Deconv_block: 2-12                [10, 256, 2]              --\n",
       "│    │    └─ConvTranspose1d: 3-25        [10, 256, 2]              393,216\n",
       "│    │    └─BatchNorm1d: 3-26            [10, 256, 2]              512\n",
       "│    │    └─ReLU: 3-27                   [10, 256, 2]              --\n",
       "│    └─Deconv_block: 2-13                [10, 256, 2]              --\n",
       "│    │    └─ConvTranspose1d: 3-28        [10, 256, 2]              196,608\n",
       "│    │    └─BatchNorm1d: 3-29            [10, 256, 2]              512\n",
       "│    │    └─ReLU: 3-30                   [10, 256, 2]              --\n",
       "│    └─Deconv_block: 2-14                [10, 256, 2]              --\n",
       "│    │    └─ConvTranspose1d: 3-31        [10, 256, 2]              196,608\n",
       "│    │    └─BatchNorm1d: 3-32            [10, 256, 2]              512\n",
       "│    │    └─ReLU: 3-33                   [10, 256, 2]              --\n",
       "│    └─Deconv_block: 2-15                [10, 128, 2]              --\n",
       "│    │    └─ConvTranspose1d: 3-34        [10, 128, 2]              98,304\n",
       "│    │    └─BatchNorm1d: 3-35            [10, 128, 2]              256\n",
       "│    │    └─ReLU: 3-36                   [10, 128, 2]              --\n",
       "│    └─Deconv_block: 2-16                [10, 64, 2]               --\n",
       "│    │    └─ConvTranspose1d: 3-37        [10, 64, 2]               24,576\n",
       "│    │    └─BatchNorm1d: 3-38            [10, 64, 2]               128\n",
       "│    │    └─ReLU: 3-39                   [10, 64, 2]               --\n",
       "│    └─Deconv_block: 2-17                [10, 32, 2]               --\n",
       "│    │    └─ConvTranspose1d: 3-40        [10, 32, 2]               6,144\n",
       "│    │    └─BatchNorm1d: 3-41            [10, 32, 2]               64\n",
       "│    │    └─ReLU: 3-42                   [10, 32, 2]               --\n",
       "==========================================================================================\n",
       "Total params: 3,111,106\n",
       "Trainable params: 3,111,106\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 224.09\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 6.63\n",
       "Params size (MB): 12.44\n",
       "Estimated Total Size (MB): 19.08\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = Spice_model2()\n",
    "summary(s, input_size=[10, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, param in s.named_parameters():\n",
    "    print(type(param), param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling_factor(Q):\n",
    "    # take care of negative inside log\n",
    "    # for Mir1k values are 666.6649397699019, 66.9456331525636\n",
    "    # since we are not using interpolated labels for training we use original \n",
    "    # dataset values for fmin and fmax\n",
    "    # Mir1k\n",
    "    fmax, fmin = 666.664939769901, 66.9456331525636256\n",
    "    # MDBSynth\n",
    "    print('inside Q', fmax, fmin)\n",
    "    return 1 / (Q * np.log2(fmax / fmin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside Q 666.664939769901 66.94563315256363\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.012565718708847283"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaling_factor(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2fed21d7f443e9082cfc6ba2fbe018596127843c8658e141820a67f60820f8cf"
  },
  "kernelspec": {
   "display_name": "Python 3.10.11 ('mpa')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
